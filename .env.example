# LangGraph Ollama Local - Environment Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# Ollama Connection Settings
# =============================================================================

# Ollama server hostname
# Use 127.0.0.1 for local, or IP address for LAN server (e.g., 192.168.1.100)
OLLAMA_HOST=127.0.0.1

# Ollama server port (default: 11434)
OLLAMA_PORT=11434

# Default model for agents
# Smaller models (1b-3b) are faster for development
# Larger models (7b+) are better for production
OLLAMA_MODEL=llama3.2:3b

# Request timeout in seconds
# Increase for slower hardware or larger models
OLLAMA_TIMEOUT=120

# Maximum retry attempts for failed requests
OLLAMA_MAX_RETRIES=3

# =============================================================================
# LangGraph Settings
# =============================================================================

# Maximum recursion depth for agent loops
# Prevents infinite loops in agent execution
LANGGRAPH_RECURSION_LIMIT=25

# Directory for storing agent state checkpoints
LANGGRAPH_CHECKPOINT_DIR=.checkpoints

# Enable token streaming by default
LANGGRAPH_ENABLE_STREAMING=true

# =============================================================================
# Development Settings
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO
