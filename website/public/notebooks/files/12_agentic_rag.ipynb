{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 12: Agentic RAG\n",
    "\n",
    "Build a RAG system where an **agent** controls retrieval - decomposing queries, performing multiple retrievals, and iteratively refining answers.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Agent-Controlled Retrieval**: LLM decides when/what to retrieve\n",
    "- **Query Decomposition**: Break complex questions into sub-queries\n",
    "- **Multi-Step Retrieval**: Multiple retrieval rounds\n",
    "- **Tool-Based RAG**: Retrieval as a tool the agent can call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Why Agentic RAG?\n",
    "\n",
    "Previous patterns use fixed retrieval flows. **Agentic RAG** lets the LLM control:\n",
    "- Whether to retrieve at all\n",
    "- What queries to use\n",
    "- When to stop retrieving\n",
    "\n",
    "```\n",
    "Agent Loop:\n",
    "  Think → Decide (retrieve/answer) → Act → Observe → Repeat\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0,\n",
    ")\n",
    "print(f\"Using model: {config.ollama.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgenticRAGState(TypedDict):\n",
    "    \"\"\"State for Agentic RAG.\"\"\"\n",
    "    messages: Annotated[list, add_messages]  # Conversation history\n",
    "    documents: List[Document]                 # Retrieved documents\n",
    "    retrieval_count: int                      # Number of retrievals done\n",
    "\n",
    "print(\"State defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local.rag import LocalRetriever\n",
    "\n",
    "retriever = LocalRetriever()\n",
    "\n",
    "# Create retrieval tool\n",
    "@tool\n",
    "def search_documents(query: str) -> str:\n",
    "    \"\"\"Search the document database for information.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find relevant documents.\n",
    "    \n",
    "    Returns:\n",
    "        Retrieved document contents.\n",
    "    \"\"\"\n",
    "    docs = retriever.retrieve_documents(query, k=3)\n",
    "    if not docs:\n",
    "        return \"No relevant documents found.\"\n",
    "    \n",
    "    results = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('filename', 'Unknown')\n",
    "        results.append(f\"[Doc {i} - {source}]:\\n{doc.page_content[:500]}...\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "tools = [search_documents]\n",
    "print(f\"Tools: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a research assistant with access to a document database.\n",
    "\n",
    "Your goal is to answer questions thoroughly using the search_documents tool.\n",
    "\n",
    "Strategy:\n",
    "1. For complex questions, break them into sub-questions\n",
    "2. Search for each aspect separately\n",
    "3. Synthesize information from multiple searches\n",
    "4. Provide a comprehensive answer with sources\n",
    "\n",
    "You can search multiple times if needed. When you have enough information, provide your final answer.\"\"\"\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"LLM with tools configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "def agent(state: AgenticRAGState) -> dict:\n",
    "    \"\"\"Agent node - decides whether to search or answer.\"\"\"\n",
    "    print(\"--- AGENT THINKING ---\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Add system message if not present\n",
    "    if not any(isinstance(m, SystemMessage) for m in messages):\n",
    "        messages = [SystemMessage(content=SYSTEM_PROMPT)] + list(messages)\n",
    "    \n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def execute_tools(state: AgenticRAGState) -> dict:\n",
    "    \"\"\"Execute any tool calls.\"\"\"\n",
    "    print(\"--- EXECUTING TOOLS ---\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    tool_results = []\n",
    "    retrieval_count = state.get(\"retrieval_count\", 0)\n",
    "    \n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        \n",
    "        print(f\"  Calling {tool_name} with: {tool_args}\")\n",
    "        \n",
    "        if tool_name == \"search_documents\":\n",
    "            result = search_documents.invoke(tool_args)\n",
    "            retrieval_count += 1\n",
    "        else:\n",
    "            result = f\"Unknown tool: {tool_name}\"\n",
    "        \n",
    "        tool_results.append(\n",
    "            ToolMessage(content=result, tool_call_id=tool_call[\"id\"])\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"messages\": tool_results,\n",
    "        \"retrieval_count\": retrieval_count,\n",
    "    }\n",
    "\n",
    "print(\"Agent nodes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgenticRAGState) -> str:\n",
    "    \"\"\"Decide whether to continue or end.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If there are tool calls, execute them\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Otherwise, we're done\n",
    "    return \"end\"\n",
    "\n",
    "print(\"Routing defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build graph\n",
    "graph = StateGraph(AgenticRAGState)\n",
    "\n",
    "graph.add_node(\"agent\", agent)\n",
    "graph.add_node(\"tools\", execute_tools)\n",
    "\n",
    "graph.add_edge(START, \"agent\")\n",
    "graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\"tools\": \"tools\", \"end\": END}\n",
    ")\n",
    "graph.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "agentic_rag = graph.compile()\n",
    "print(\"Agentic RAG compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(agentic_rag.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    print(agentic_rag.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a complex question\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "question = \"Compare Self-RAG and CRAG. What are the key differences and when should I use each?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = agentic_rag.invoke({\n",
    "    \"messages\": [HumanMessage(content=question)],\n",
    "    \"retrieval_count\": 0,\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nRetrievals performed: {result['retrieval_count']}\")\n",
    "print(f\"\\nFinal Answer:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **Tool-based Retrieval** | Agent decides when to search |\n",
    "| **ReAct Loop** | Think → Act → Observe cycle |\n",
    "| **Multi-step** | Multiple retrievals for complex queries |\n",
    "| **Query Decomposition** | Agent breaks down questions |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In [Tutorial 13: Perplexity Clone](13_perplexity_clone.ipynb), you'll build a full research assistant with:\n",
    "- In-text citations\n",
    "- Source metadata\n",
    "- Streaming responses\n",
    "- Follow-up suggestions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
