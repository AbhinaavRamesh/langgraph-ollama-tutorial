{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 23: Reflexion Pattern\n",
    "\n",
    "In this tutorial, you'll learn how to implement the **Reflexion pattern** for iterative answer improvement with episodic memory. Unlike simple reflection that improves a single output, Reflexion learns from multiple attempts across iterations.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Difference between Reflection and Reflexion patterns\n",
    "- Creating structured self-critique with missing/superfluous analysis\n",
    "- Using episodic memory to learn from past attempts\n",
    "- Executing search queries to gather missing information\n",
    "- Revising answers based on reflection and search results\n",
    "- Building the complete Reflexion loop\n",
    "\n",
    "By the end, you'll have a working Reflexion agent that improves answers through iterative learning and external search.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of basic LangGraph patterns\n",
    "- Ollama running with a capable model (llama3.1:8b or larger recommended)\n",
    "- (Optional) Tavily API key for search functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Why Reflexion?\n",
    "\n",
    "Traditional agents answer questions once and move on. Even with reflection, they improve a single answer iteratively. **Reflexion** goes further by:\n",
    "\n",
    "1. **Learning across attempts**: Maintains episodic memory of all previous tries\n",
    "2. **Self-critique**: Identifies what's missing and what's superfluous\n",
    "3. **External search**: Actively seeks information to fill knowledge gaps\n",
    "4. **Iterative refinement**: Each attempt learns from all previous failures\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "| Pattern | Memory | Search | Use Case |\n",
    "|---------|--------|--------|----------|\n",
    "| **Reflection** | Single output | No | Improve writing quality |\n",
    "| **Reflexion** | All attempts | Yes | Research questions, knowledge gaps |\n",
    "\n",
    "```\n",
    "┌───────────────────────────────────────────────────────────┐\n",
    "│                   Reflexion Loop                           │\n",
    "│                                                             │\n",
    "│  ┌──────────┐    ┌────────────┐    ┌─────────────┐       │\n",
    "│  │  Draft   │───►│  Execute   │───►│   Revise    │       │\n",
    "│  │ Answer + │    │  Search    │    │  Based on   │       │\n",
    "│  │ Reflect  │    │  Queries   │    │  Results    │       │\n",
    "│  └────┬─────┘    └────────────┘    └──────┬──────┘       │\n",
    "│       │                                     │               │\n",
    "│       │          Episodic Memory            │               │\n",
    "│       └─────────────────────────────────────┘               │\n",
    "│                    (Loop until max iterations)              │\n",
    "└───────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our setup\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "print(f\"Ollama URL: {config.ollama.base_url}\")\n",
    "print(f\"Model: {config.ollama.model}\")\n",
    "print(\"Setup verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Define the Reflexion State\n",
    "\n",
    "The key difference from Reflection: we maintain **episodic memory** of ALL attempts and reflections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "\n",
    "\n",
    "class ReflexionState(TypedDict):\n",
    "    \"\"\"State for Reflexion pattern with episodic memory.\"\"\"\n",
    "    \n",
    "    # Message history (for tool execution)\n",
    "    messages: Annotated[list, add_messages]\n",
    "    \n",
    "    # The task/question to solve\n",
    "    task: str\n",
    "    \n",
    "    # ALL attempts - uses operator.add to accumulate\n",
    "    attempts: Annotated[list[dict], operator.add]\n",
    "    \n",
    "    # Current attempt being worked on\n",
    "    current_attempt: str\n",
    "    \n",
    "    # ALL reflections - uses operator.add to accumulate\n",
    "    reflections: Annotated[list[str], operator.add]\n",
    "    \n",
    "    # Latest reflection\n",
    "    current_reflection: str\n",
    "    \n",
    "    # Iteration tracking\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    \n",
    "    # Success flag\n",
    "    success_achieved: bool\n",
    "\n",
    "\n",
    "print(\"Reflexion state defined!\")\n",
    "print(\"\\nKey fields:\")\n",
    "print(\"- attempts: Accumulates ALL attempts (episodic memory)\")\n",
    "print(\"- reflections: Accumulates ALL self-critiques\")\n",
    "print(\"- iteration: Tracks current iteration number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Define Structured Output Models\n",
    "\n",
    "We use Pydantic models for reliable self-critique and answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    \"\"\"Structured self-critique of an answer.\"\"\"\n",
    "    \n",
    "    missing: str = Field(\n",
    "        description=\"What critical information is missing or incomplete?\"\n",
    "    )\n",
    "    superfluous: str = Field(\n",
    "        description=\"What information is unnecessary or irrelevant?\"\n",
    "    )\n",
    "\n",
    "\n",
    "class AnswerQuestion(BaseModel):\n",
    "    \"\"\"Structured answer with self-critique and search queries.\"\"\"\n",
    "    \n",
    "    answer: str = Field(\n",
    "        description=\"Your answer to the question (~250 words)\"\n",
    "    )\n",
    "    reflection: Reflection = Field(\n",
    "        description=\"Self-critique identifying gaps and excess\"\n",
    "    )\n",
    "    search_queries: list[str] = Field(\n",
    "        description=\"1-3 search queries to fill knowledge gaps\",\n",
    "        max_length=3,\n",
    "    )\n",
    "\n",
    "\n",
    "class ReviseAnswer(AnswerQuestion):\n",
    "    \"\"\"Revised answer with citations.\"\"\"\n",
    "    \n",
    "    references: list[str] = Field(\n",
    "        description=\"Sources and citations used\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Structured output models defined!\")\n",
    "print(\"\\nModels:\")\n",
    "print(\"- Reflection: missing + superfluous analysis\")\n",
    "print(\"- AnswerQuestion: answer + reflection + search_queries\")\n",
    "print(\"- ReviseAnswer: extends AnswerQuestion with references\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 4: Create the Initial Responder Node\n",
    "\n",
    "This node generates the first answer attempt with immediate self-critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "\n",
    "def create_initial_responder(llm):\n",
    "    \"\"\"Create initial responder that drafts answer with self-critique.\"\"\"\n",
    "    \n",
    "    # Try structured output\n",
    "    try:\n",
    "        structured_llm = llm.with_structured_output(AnswerQuestion)\n",
    "        use_structured = True\n",
    "    except (AttributeError, NotImplementedError):\n",
    "        structured_llm = llm\n",
    "        use_structured = False\n",
    "    \n",
    "    def responder(state: ReflexionState) -> dict:\n",
    "        \"\"\"Generate initial answer with reflection.\"\"\"\n",
    "        task = state[\"task\"]\n",
    "        iteration = state.get(\"iteration\", 0)\n",
    "        previous_attempts = state.get(\"attempts\", [])\n",
    "        previous_reflections = state.get(\"reflections\", [])\n",
    "        \n",
    "        # Build prompt with episodic memory\n",
    "        prompt = f\"\"\"Answer the following question thoughtfully.\\n\\nQuestion: {task}\"\"\"\n",
    "        \n",
    "        # Include previous attempts if available\n",
    "        if previous_attempts and previous_reflections:\n",
    "            prompt += \"\"\"\\n\\nPrevious attempts and what was wrong:\"\"\"\n",
    "            for i, (attempt, reflection) in enumerate(zip(previous_attempts, previous_reflections), 1):\n",
    "                prompt += f\"\"\"\\n\\nAttempt {i}:\n",
    "Answer: {attempt.get('answer', 'N/A')[:150]}...\n",
    "Issues: {reflection[:150]}...\"\"\"\n",
    "            prompt += \"\"\"\\n\\nLearn from these mistakes. Avoid repeating them.\"\"\"\n",
    "        \n",
    "        prompt += \"\"\"\\n\\nAfter answering:\n",
    "1. Critique your answer - what's missing? What's unnecessary?\n",
    "2. Suggest 1-3 search queries to fill knowledge gaps.\"\"\"\n",
    "        \n",
    "        if use_structured:\n",
    "            try:\n",
    "                response = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "                answer = response.answer\n",
    "                reflection_obj = response.reflection\n",
    "                search_queries = response.search_queries\n",
    "            except Exception:\n",
    "                response = llm.invoke([HumanMessage(content=prompt)])\n",
    "                answer = response.content\n",
    "                reflection_obj = Reflection(\n",
    "                    missing=\"Unable to determine\",\n",
    "                    superfluous=\"Unable to determine\"\n",
    "                )\n",
    "                search_queries = [task]\n",
    "        else:\n",
    "            response = llm.invoke([HumanMessage(content=prompt)])\n",
    "            answer = response.content\n",
    "            reflection_obj = Reflection(\n",
    "                missing=\"Manual review needed\",\n",
    "                superfluous=\"Manual review needed\"\n",
    "            )\n",
    "            search_queries = [task]\n",
    "        \n",
    "        reflection_text = f\"Missing: {reflection_obj.missing}\\nSuperfluous: {reflection_obj.superfluous}\"\n",
    "        \n",
    "        return {\n",
    "            \"current_attempt\": answer,\n",
    "            \"current_reflection\": reflection_text,\n",
    "            \"attempts\": [{\n",
    "                \"num\": iteration + 1,\n",
    "                \"answer\": answer,\n",
    "                \"search_queries\": search_queries,\n",
    "            }],\n",
    "            \"iteration\": iteration + 1,\n",
    "        }\n",
    "    \n",
    "    return responder\n",
    "\n",
    "\n",
    "print(\"Initial responder node creator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 5: Create a Mock Search Tool\n",
    "\n",
    "For demonstration, we'll use a mock search tool. In production, use TavilySearchResults or DuckDuckGoSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "from typing import Type, Optional\n",
    "from pydantic import BaseModel as PydanticBaseModel, Field as PydanticField\n",
    "\n",
    "\n",
    "class SearchInput(PydanticBaseModel):\n",
    "    \"\"\"Input for the search tool.\"\"\"\n",
    "    query: str = PydanticField(description=\"The search query\")\n",
    "\n",
    "\n",
    "class MockSearchTool(BaseTool):\n",
    "    \"\"\"Mock search tool for demonstration.\"\"\"\n",
    "    \n",
    "    name: str = \"search\"\n",
    "    description: str = \"Search for information on the web\"\n",
    "    args_schema: Type[PydanticBaseModel] = SearchInput\n",
    "    \n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"Mock search that returns generic results.\"\"\"\n",
    "        return f\"\"\"Search results for: {query}\n",
    "\n",
    "Result 1: Recent developments in {query} show significant progress.\n",
    "Result 2: Experts in the field emphasize the importance of understanding {query}.\n",
    "Result 3: Latest research indicates new applications of {query} in various domains.\"\"\"\n",
    "    \n",
    "    async def _arun(self, query: str) -> str:\n",
    "        \"\"\"Async version.\"\"\"\n",
    "        return self._run(query)\n",
    "\n",
    "\n",
    "search_tool = MockSearchTool()\n",
    "print(\"Mock search tool created!\")\n",
    "print(\"\\nNote: In production, use:\")\n",
    "print(\"  from langchain_community.tools.tavily_search import TavilySearchResults\")\n",
    "print(\"  search_tool = TavilySearchResults(max_results=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 6: Create the Tool Executor Node\n",
    "\n",
    "This node executes the search queries generated during reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "def create_tool_executor(search_tool):\n",
    "    \"\"\"Create tool executor that runs search queries.\"\"\"\n",
    "    \n",
    "    def executor(state: ReflexionState) -> dict:\n",
    "        \"\"\"Execute search queries from last attempt.\"\"\"\n",
    "        attempts = state.get(\"attempts\", [])\n",
    "        \n",
    "        if not attempts:\n",
    "            return {\"messages\": [ToolMessage(\n",
    "                content=\"No attempts available\",\n",
    "                tool_call_id=\"search\"\n",
    "            )]}\n",
    "        \n",
    "        last_attempt = attempts[-1]\n",
    "        queries = last_attempt.get(\"search_queries\", [])\n",
    "        \n",
    "        if not queries:\n",
    "            return {\"messages\": [ToolMessage(\n",
    "                content=\"No search queries generated\",\n",
    "                tool_call_id=\"search\"\n",
    "            )]}\n",
    "        \n",
    "        # Execute each query\n",
    "        all_results = []\n",
    "        for query in queries:\n",
    "            try:\n",
    "                result = search_tool.invoke(query)\n",
    "                all_results.append(f\"Query: {query}\\nResults: {result}\\n\")\n",
    "            except Exception as e:\n",
    "                all_results.append(f\"Query: {query}\\nError: {str(e)}\\n\")\n",
    "        \n",
    "        combined_results = \"\\n---\\n\".join(all_results)\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [ToolMessage(\n",
    "                content=combined_results,\n",
    "                tool_call_id=\"search\"\n",
    "            )],\n",
    "        }\n",
    "    \n",
    "    return executor\n",
    "\n",
    "\n",
    "print(\"Tool executor node creator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 7: Create the Revisor Node\n",
    "\n",
    "This node revises the answer using search results and reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_revisor(llm):\n",
    "    \"\"\"Create revisor that improves answer using search results.\"\"\"\n",
    "    \n",
    "    # Try structured output\n",
    "    try:\n",
    "        structured_llm = llm.with_structured_output(ReviseAnswer)\n",
    "        use_structured = True\n",
    "    except (AttributeError, NotImplementedError):\n",
    "        structured_llm = llm\n",
    "        use_structured = False\n",
    "    \n",
    "    def revisor(state: ReflexionState) -> dict:\n",
    "        \"\"\"Revise answer using search results and reflection.\"\"\"\n",
    "        task = state[\"task\"]\n",
    "        current_reflection = state.get(\"current_reflection\", \"\")\n",
    "        messages = state.get(\"messages\", [])\n",
    "        \n",
    "        # Get search results\n",
    "        search_results = \"No search results available\"\n",
    "        if messages:\n",
    "            for msg in reversed(messages):\n",
    "                if isinstance(msg, ToolMessage):\n",
    "                    search_results = msg.content\n",
    "                    break\n",
    "        \n",
    "        prompt = f\"\"\"Revise your previous answer using new information.\n",
    "\n",
    "Question: {task}\n",
    "\n",
    "Previous reflection:\n",
    "{current_reflection}\n",
    "\n",
    "Search results:\n",
    "{search_results}\n",
    "\n",
    "Instructions:\n",
    "1. Incorporate relevant information from search results\n",
    "2. Address gaps identified in reflection\n",
    "3. Remove unnecessary information\n",
    "4. Include citations/references\n",
    "5. Keep answer around 250 words\n",
    "\n",
    "Provide revised answer with references.\"\"\"\n",
    "        \n",
    "        if use_structured:\n",
    "            try:\n",
    "                response = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "                revised_answer = response.answer\n",
    "                new_reflection = response.reflection\n",
    "                references = response.references\n",
    "            except Exception:\n",
    "                response = llm.invoke([HumanMessage(content=prompt)])\n",
    "                revised_answer = response.content\n",
    "                new_reflection = Reflection(\n",
    "                    missing=\"Unable to determine\",\n",
    "                    superfluous=\"Unable to determine\"\n",
    "                )\n",
    "                references = [\"Search results incorporated\"]\n",
    "        else:\n",
    "            response = llm.invoke([HumanMessage(content=prompt)])\n",
    "            revised_answer = response.content\n",
    "            new_reflection = Reflection(\n",
    "                missing=\"Manual review needed\",\n",
    "                superfluous=\"Manual review needed\"\n",
    "            )\n",
    "            references = [\"See search results above\"]\n",
    "        \n",
    "        reflection_text = f\"Missing: {new_reflection.missing}\\nSuperfluous: {new_reflection.superfluous}\"\n",
    "        iteration = state.get(\"iteration\", 0)\n",
    "        \n",
    "        return {\n",
    "            \"current_attempt\": revised_answer,\n",
    "            \"attempts\": [{\n",
    "                \"num\": iteration + 1,\n",
    "                \"answer\": revised_answer,\n",
    "                \"references\": references,\n",
    "            }],\n",
    "            \"reflections\": [current_reflection],\n",
    "            \"current_reflection\": reflection_text,\n",
    "        }\n",
    "    \n",
    "    return revisor\n",
    "\n",
    "\n",
    "print(\"Revisor node creator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 8: Build the Reflexion Graph\n",
    "\n",
    "Now we assemble all pieces into the Reflexion loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "def create_reflexion_graph(llm, search_tool):\n",
    "    \"\"\"Build Reflexion graph for iterative improvement.\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(ReflexionState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"draft\", create_initial_responder(llm))\n",
    "    workflow.add_node(\"execute_tools\", create_tool_executor(search_tool))\n",
    "    workflow.add_node(\"revise\", create_revisor(llm))\n",
    "    \n",
    "    # Entry: draft initial answer\n",
    "    workflow.add_edge(START, \"draft\")\n",
    "    \n",
    "    # Draft -> Execute tools\n",
    "    workflow.add_edge(\"draft\", \"execute_tools\")\n",
    "    \n",
    "    # Execute tools -> Revise\n",
    "    workflow.add_edge(\"execute_tools\", \"revise\")\n",
    "    \n",
    "    # Conditional: continue or end\n",
    "    def should_continue(state: ReflexionState) -> str:\n",
    "        \"\"\"Determine whether to continue iterating.\"\"\"\n",
    "        iteration = state.get(\"iteration\", 0)\n",
    "        max_iterations = state.get(\"max_iterations\", 3)\n",
    "        success = state.get(\"success_achieved\", False)\n",
    "        \n",
    "        if success or iteration >= max_iterations:\n",
    "            return END\n",
    "        \n",
    "        return \"draft\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"revise\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"draft\": \"draft\",\n",
    "            END: END,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "reflexion_graph = create_reflexion_graph(llm, search_tool)\n",
    "\n",
    "print(\"Reflexion graph compiled!\")\n",
    "print(\"\\nGraph flow:\")\n",
    "print(\"  START -> draft\")\n",
    "print(\"  draft -> execute_tools\")\n",
    "print(\"  execute_tools -> revise\")\n",
    "print(\"  revise -> [draft | END]\")\n",
    "print(\"\\nLoop continues until max_iterations or success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 9: Run a Reflexion Task\n",
    "\n",
    "Let's test our Reflexion agent on a research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reflexion_task(graph, task: str, max_iterations: int = 3):\n",
    "    \"\"\"Run a Reflexion task.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"REFLEXION TASK\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Question: {task}\")\n",
    "    print(f\"Max iterations: {max_iterations}\")\n",
    "    \n",
    "    initial_state: ReflexionState = {\n",
    "        \"messages\": [],\n",
    "        \"task\": task,\n",
    "        \"attempts\": [],\n",
    "        \"current_attempt\": \"\",\n",
    "        \"reflections\": [],\n",
    "        \"current_reflection\": \"\",\n",
    "        \"iteration\": 0,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"success_achieved\": False,\n",
    "    }\n",
    "    \n",
    "    result = graph.invoke(initial_state)\n",
    "    \n",
    "    # Display all attempts\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALL ATTEMPTS (Episodic Memory)\")\n",
    "    print(\"=\"*60)\n",
    "    for attempt in result[\"attempts\"]:\n",
    "        print(f\"\\n--- Attempt {attempt['num']} ---\")\n",
    "        print(f\"Answer: {attempt['answer'][:300]}...\")\n",
    "        if 'search_queries' in attempt:\n",
    "            print(f\"Search queries: {attempt['search_queries']}\")\n",
    "        if 'references' in attempt:\n",
    "            print(f\"References: {attempt['references']}\")\n",
    "    \n",
    "    # Display all reflections\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALL REFLECTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    for i, reflection in enumerate(result[\"reflections\"], 1):\n",
    "        print(f\"\\nReflection {i}:\")\n",
    "        print(reflection)\n",
    "    \n",
    "    # Display final answer\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL ANSWER\")\n",
    "    print(\"=\"*60)\n",
    "    print(result[\"current_attempt\"])\n",
    "    \n",
    "    print(f\"\\nTotal iterations: {result['iteration']}\")\n",
    "    print(f\"Total attempts: {len(result['attempts'])}\")\n",
    "    print(f\"Total reflections: {len(result['reflections'])}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Run a task\n",
    "result = run_reflexion_task(\n",
    "    reflexion_graph,\n",
    "    task=\"What are the key applications of quantum computing?\",\n",
    "    max_iterations=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 10: Observe Episodic Memory\n",
    "\n",
    "Let's examine how Reflexion accumulates knowledge across attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the episodic memory structure\n",
    "print(\"Episodic Memory Structure:\")\n",
    "print(\"\\nAttempts:\")\n",
    "for i, attempt in enumerate(result[\"attempts\"], 1):\n",
    "    print(f\"\\nAttempt {i}:\")\n",
    "    print(f\"  - Number: {attempt['num']}\")\n",
    "    print(f\"  - Has answer: {len(attempt.get('answer', '')) > 0}\")\n",
    "    print(f\"  - Has search queries: {'search_queries' in attempt}\")\n",
    "    print(f\"  - Has references: {'references' in attempt}\")\n",
    "\n",
    "print(\"\\nReflections:\")\n",
    "for i, reflection in enumerate(result[\"reflections\"], 1):\n",
    "    print(f\"\\nReflection {i}:\")\n",
    "    lines = reflection.split('\\n')\n",
    "    for line in lines:\n",
    "        print(f\"  {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Step 11: Try Different Questions\n",
    "\n",
    "Test Reflexion with various types of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question requiring technical depth\n",
    "result2 = run_reflexion_task(\n",
    "    reflexion_graph,\n",
    "    task=\"Explain how CRISPR gene editing works and its current limitations.\",\n",
    "    max_iterations=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question requiring recent information\n",
    "result3 = run_reflexion_task(\n",
    "    reflexion_graph,\n",
    "    task=\"What are the latest developments in renewable energy storage?\",\n",
    "    max_iterations=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Step 12: Using the Built-in Module\n",
    "\n",
    "The `langgraph_ollama_local.patterns` module provides ready-to-use Reflexion functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local.patterns.reflexion import (\n",
    "    create_reflexion_graph,\n",
    "    run_reflexion_task,\n",
    ")\n",
    "\n",
    "# Use the module's implementation\n",
    "module_graph = create_reflexion_graph(llm, search_tool)\n",
    "\n",
    "result = run_reflexion_task(\n",
    "    module_graph,\n",
    "    task=\"What are the ethical implications of artificial general intelligence?\",\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(result[\"current_attempt\"])\n",
    "print(f\"\\nTotal attempts: {len(result['attempts'])}\")\n",
    "print(f\"Total reflections: {len(result['reflections'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Step 13: Compare Reflection vs Reflexion\n",
    "\n",
    "Let's highlight the key differences in a side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"REFLECTION vs REFLEXION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = [\n",
    "    (\"Memory\", \"Single output\", \"All attempts (episodic)\"),\n",
    "    (\"Search\", \"No external search\", \"Active search for info\"),\n",
    "    (\"Learning\", \"Improve one answer\", \"Learn from failures\"),\n",
    "    (\"Iterations\", \"Generate-critique-revise\", \"Draft-search-revise\"),\n",
    "    (\"Use Case\", \"Writing quality\", \"Research questions\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Aspect':<15} {'Reflection':<25} {'Reflexion':<25}\")\n",
    "print(\"-\" * 65)\n",
    "for aspect, reflection, reflexion in comparison:\n",
    "    print(f\"{aspect:<15} {reflection:<25} {reflexion:<25}\")\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"Reflexion's episodic memory allows it to avoid repeating\")\n",
    "print(\"the same mistakes across attempts, making it ideal for\")\n",
    "print(\"complex research questions requiring external information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Complete Code\n",
    "\n",
    "Here's the complete implementation in one cell for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Complete Reflexion Implementation ===\n",
    "\n",
    "from typing import Annotated, Type\n",
    "from typing_extensions import TypedDict\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "\n",
    "# === State ===\n",
    "class ReflexionState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    task: str\n",
    "    attempts: Annotated[list[dict], operator.add]\n",
    "    current_attempt: str\n",
    "    reflections: Annotated[list[str], operator.add]\n",
    "    current_reflection: str\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    success_achieved: bool\n",
    "\n",
    "\n",
    "# === Models ===\n",
    "class Reflection(BaseModel):\n",
    "    missing: str\n",
    "    superfluous: str\n",
    "\n",
    "\n",
    "class AnswerQuestion(BaseModel):\n",
    "    answer: str\n",
    "    reflection: Reflection\n",
    "    search_queries: list[str]\n",
    "\n",
    "\n",
    "# === Quick Example ===\n",
    "def quick_reflexion_example():\n",
    "    config = LocalAgentConfig()\n",
    "    llm = ChatOllama(model=config.ollama.model, base_url=config.ollama.base_url)\n",
    "    \n",
    "    # Mock search tool\n",
    "    class MockSearch(BaseTool):\n",
    "        name: str = \"search\"\n",
    "        description: str = \"Search tool\"\n",
    "        \n",
    "        def _run(self, query: str) -> str:\n",
    "            return f\"Mock results for: {query}\"\n",
    "    \n",
    "    from langgraph_ollama_local.patterns.reflexion import (\n",
    "        create_reflexion_graph,\n",
    "        run_reflexion_task,\n",
    "    )\n",
    "    \n",
    "    graph = create_reflexion_graph(llm, MockSearch())\n",
    "    result = run_reflexion_task(\n",
    "        graph,\n",
    "        task=\"Explain quantum computing\",\n",
    "        max_iterations=2\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = quick_reflexion_example()\n",
    "    print(f\"Total attempts: {len(result['attempts'])}\")\n",
    "    print(f\"Final answer length: {len(result['current_attempt'])} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Episodic Memory** | Stores ALL attempts to learn from failures |\n",
    "| **Self-Reflection** | Identifies missing and superfluous information |\n",
    "| **Search Integration** | Actively queries external sources |\n",
    "| **Iterative Learning** | Each attempt improves on previous ones |\n",
    "| **Structured Output** | Pydantic models for reliable critique |\n",
    "| **operator.add** | Accumulates attempts and reflections |\n",
    "| **Tool Messages** | Store search results in message history |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Set reasonable iteration limits**: 2-3 iterations usually sufficient\n",
    "2. **Use quality search tools**: TavilySearchResults or DuckDuckGoSearch in production\n",
    "3. **Monitor episodic memory**: Check that previous attempts inform new ones\n",
    "4. **Structured output crucial**: Ensures reliable reflection parsing\n",
    "5. **Limit search queries**: 1-3 queries per iteration prevents overload\n",
    "6. **Track references**: Citations improve answer credibility\n",
    "7. **Compare attempts**: Review how answers improve across iterations\n",
    "\n",
    "## What's Next\n",
    "\n",
    "Congratulations! You've implemented the Reflexion pattern. You now understand:\n",
    "- How episodic memory enables learning from failures\n",
    "- Structured self-critique with missing/superfluous analysis\n",
    "- Integration of external search for knowledge gaps\n",
    "- Iterative refinement with accumulated reflections\n",
    "\n",
    "Continue exploring:\n",
    "- Tutorial 24: LATS (Tree search for agents)\n",
    "- Tutorial 25: ReWOO (Decoupled planning)\n",
    "- Combine Reflexion with evaluation patterns\n",
    "- Use real search APIs for production applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
