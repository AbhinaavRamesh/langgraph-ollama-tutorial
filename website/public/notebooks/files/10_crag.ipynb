{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 10: CRAG (Corrective RAG)\n",
    "\n",
    "In this tutorial, you'll build a **Corrective RAG** system that uses web search as a fallback when local document retrieval is insufficient.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Knowledge Assessment**: Determine if retrieved documents are sufficient\n",
    "- **Web Search Fallback**: Use external search when needed\n",
    "- **Knowledge Fusion**: Combine local and web sources\n",
    "- **Corrective Flow**: Fix retrieval failures dynamically\n",
    "\n",
    "By the end, you'll have a robust RAG system that never says \"I don't know\" when the answer exists online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Why CRAG?\n",
    "\n",
    "Self-RAG grades documents but can't fix poor retrieval. **CRAG** adds correction:\n",
    "\n",
    "```\n",
    "                    ┌─── Local docs sufficient ───▶ Generate\n",
    "Retrieve → Grade ──┤\n",
    "                    └─── Insufficient ───▶ Web Search ───▶ Generate\n",
    "```\n",
    "\n",
    "This is particularly useful when:\n",
    "- Your document corpus doesn't cover the topic\n",
    "- The user asks about recent events\n",
    "- You need to supplement local knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "For web search, we'll use Tavily (free tier available) or DuckDuckGo:\n",
    "```bash\n",
    "pip install tavily-python duckduckgo-search\n",
    "```\n",
    "\n",
    "Set your API key (optional, we have a mock fallback):\n",
    "```bash\n",
    "export TAVILY_API_KEY=\"your-key-here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Check for Tavily API key\n",
    "has_tavily = bool(os.environ.get(\"TAVILY_API_KEY\"))\n",
    "print(f\"Using model: {config.ollama.model}\")\n",
    "print(f\"Tavily API available: {has_tavily}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 1: Define the State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class CRAGState(TypedDict):\n",
    "    \"\"\"State for Corrective RAG pipeline.\"\"\"\n",
    "    question: str                      # User's question\n",
    "    documents: List[Document]          # Retrieved local documents\n",
    "    web_results: List[Document]        # Web search results\n",
    "    combined_documents: List[Document] # Merged documents for generation\n",
    "    knowledge_source: str              # \"local\", \"web\", or \"combined\"\n",
    "    generation: str                    # Final answer\n",
    "\n",
    "print(\"State defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Create Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(query: str, max_results: int = 3) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Search the web for information.\n",
    "    Uses Tavily if available, otherwise falls back to mock results.\n",
    "    \"\"\"\n",
    "    # Try Tavily first\n",
    "    if os.environ.get(\"TAVILY_API_KEY\"):\n",
    "        try:\n",
    "            from tavily import TavilyClient\n",
    "            client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "            response = client.search(query, max_results=max_results)\n",
    "            \n",
    "            return [\n",
    "                Document(\n",
    "                    page_content=r.get(\"content\", \"\"),\n",
    "                    metadata={\n",
    "                        \"source\": r.get(\"url\", \"\"),\n",
    "                        \"title\": r.get(\"title\", \"\"),\n",
    "                        \"type\": \"web\",\n",
    "                    }\n",
    "                )\n",
    "                for r in response.get(\"results\", [])\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"Tavily search failed: {e}\")\n",
    "    \n",
    "    # Try DuckDuckGo as fallback\n",
    "    try:\n",
    "        from duckduckgo_search import DDGS\n",
    "        with DDGS() as ddgs:\n",
    "            results = list(ddgs.text(query, max_results=max_results))\n",
    "            return [\n",
    "                Document(\n",
    "                    page_content=r.get(\"body\", \"\"),\n",
    "                    metadata={\n",
    "                        \"source\": r.get(\"href\", \"\"),\n",
    "                        \"title\": r.get(\"title\", \"\"),\n",
    "                        \"type\": \"web\",\n",
    "                    }\n",
    "                )\n",
    "                for r in results\n",
    "            ]\n",
    "    except Exception as e:\n",
    "        print(f\"DuckDuckGo search failed: {e}\")\n",
    "    \n",
    "    # Mock fallback for demonstration\n",
    "    print(\"Using mock web search results\")\n",
    "    return [\n",
    "        Document(\n",
    "            page_content=f\"Mock web result for: {query}. This would contain actual search results in production.\",\n",
    "            metadata={\"source\": \"https://example.com\", \"type\": \"web_mock\"}\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Test web search\n",
    "test_results = web_search(\"What is RAG in AI?\", max_results=2)\n",
    "print(f\"Web search returned {len(test_results)} results\")\n",
    "for r in test_results:\n",
    "    print(f\"  - {r.metadata.get('title', 'No title')[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Setup Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local.rag import LocalRetriever, DocumentGrader\n",
    "\n",
    "# Local retriever\n",
    "retriever = LocalRetriever()\n",
    "\n",
    "# Document grader\n",
    "doc_grader = DocumentGrader(llm)\n",
    "\n",
    "print(\"Components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Define Node Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_local(state: CRAGState) -> dict:\n",
    "    \"\"\"Retrieve from local document store.\"\"\"\n",
    "    print(\"--- RETRIEVE LOCAL ---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.retrieve_documents(question, k=4)\n",
    "    print(f\"Retrieved {len(docs)} local documents\")\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "print(\"Local retrieve node defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state: CRAGState) -> dict:\n",
    "    \"\"\"Grade documents and decide if web search is needed.\"\"\"\n",
    "    print(\"--- GRADE DOCUMENTS ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"No documents retrieved, need web search\")\n",
    "        return {\n",
    "            \"combined_documents\": [],\n",
    "            \"knowledge_source\": \"web\",\n",
    "        }\n",
    "    \n",
    "    # Grade each document\n",
    "    relevant, irrelevant = doc_grader.grade_documents(documents, question)\n",
    "    print(f\"Relevant: {len(relevant)}, Irrelevant: {len(irrelevant)}\")\n",
    "    \n",
    "    # Decide based on relevance\n",
    "    if len(relevant) >= 2:\n",
    "        # Enough relevant documents\n",
    "        return {\n",
    "            \"combined_documents\": relevant,\n",
    "            \"knowledge_source\": \"local\",\n",
    "        }\n",
    "    elif len(relevant) == 1:\n",
    "        # Some relevant, supplement with web\n",
    "        return {\n",
    "            \"combined_documents\": relevant,\n",
    "            \"knowledge_source\": \"combined\",\n",
    "        }\n",
    "    else:\n",
    "        # No relevant documents, use web\n",
    "        return {\n",
    "            \"combined_documents\": [],\n",
    "            \"knowledge_source\": \"web\",\n",
    "        }\n",
    "\n",
    "print(\"Grade documents node defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(state: CRAGState) -> dict:\n",
    "    \"\"\"Search the web for additional information.\"\"\"\n",
    "    print(\"--- WEB SEARCH ---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    web_docs = web_search(question, max_results=3)\n",
    "    print(f\"Found {len(web_docs)} web results\")\n",
    "    \n",
    "    # Combine with any existing relevant docs\n",
    "    existing = state.get(\"combined_documents\", [])\n",
    "    combined = existing + web_docs\n",
    "    \n",
    "    return {\n",
    "        \"web_results\": web_docs,\n",
    "        \"combined_documents\": combined,\n",
    "    }\n",
    "\n",
    "print(\"Web search node defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "CRAG_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant answering questions using provided context.\n",
    "The context may come from local documents, web search, or both.\n",
    "Use the information to provide an accurate, well-sourced answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a comprehensive answer based on the context. If citing web sources, mention them.\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "def generate(state: CRAGState) -> dict:\n",
    "    \"\"\"Generate answer from combined documents.\"\"\"\n",
    "    print(\"--- GENERATE ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"combined_documents\"]\n",
    "    source = state[\"knowledge_source\"]\n",
    "    \n",
    "    print(f\"Using {len(documents)} documents from: {source}\")\n",
    "    \n",
    "    if not documents:\n",
    "        return {\n",
    "            \"generation\": \"I could not find any relevant information to answer this question.\"\n",
    "        }\n",
    "    \n",
    "    # Format context with source attribution\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        source_type = doc.metadata.get(\"type\", \"local\")\n",
    "        source_name = doc.metadata.get(\"filename\", doc.metadata.get(\"title\", \"Unknown\"))\n",
    "        context_parts.append(f\"[Source {i} ({source_type}): {source_name}]\\n{doc.page_content}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Generate\n",
    "    messages = CRAG_PROMPT.format_messages(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return {\"generation\": response.content}\n",
    "\n",
    "print(\"Generate node defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 5: Define Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_grading(state: CRAGState) -> Literal[\"generate\", \"web_search\"]:\n",
    "    \"\"\"Route based on knowledge source decision.\"\"\"\n",
    "    source = state[\"knowledge_source\"]\n",
    "    \n",
    "    if source == \"local\":\n",
    "        print(\"→ Local documents sufficient, generating...\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(f\"→ Need web search (source: {source})\")\n",
    "        return \"web_search\"\n",
    "\n",
    "print(\"Routing defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 6: Build the CRAG Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build graph\n",
    "graph_builder = StateGraph(CRAGState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"retrieve_local\", retrieve_local)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "graph_builder.add_node(\"web_search\", search_web)\n",
    "graph_builder.add_node(\"generate\", generate)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"retrieve_local\")\n",
    "graph_builder.add_edge(\"retrieve_local\", \"grade_documents\")\n",
    "\n",
    "# Conditional edge after grading\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    route_after_grading,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"web_search\": \"web_search\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Web search leads to generate\n",
    "graph_builder.add_edge(\"web_search\", \"generate\")\n",
    "\n",
    "# Generate leads to end\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "crag_graph = graph_builder.compile()\n",
    "\n",
    "print(\"CRAG graph compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(crag_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(crag_graph.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 7: Test CRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a question covered by local docs\n",
    "question1 = \"What is Self-RAG and how does it work?\"\n",
    "\n",
    "print(f\"Question: {question1}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result1 = crag_graph.invoke({\"question\": question1})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nKnowledge source: {result1['knowledge_source']}\")\n",
    "print(f\"\\nAnswer:\\n{result1['generation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a question NOT in local docs (triggers web search)\n",
    "question2 = \"What are the latest developments in AI in 2024?\"\n",
    "\n",
    "print(f\"Question: {question2}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result2 = crag_graph.invoke({\"question\": question2})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nKnowledge source: {result2['knowledge_source']}\")\n",
    "print(f\"Web results used: {len(result2.get('web_results', []))}\")\n",
    "print(f\"\\nAnswer:\\n{result2['generation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Complete CRAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete CRAG Implementation\n",
    "\n",
    "from typing import List, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "from langgraph_ollama_local.rag import LocalRetriever, DocumentGrader\n",
    "\n",
    "# State\n",
    "class CRAGState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    web_results: List[Document]\n",
    "    combined_documents: List[Document]\n",
    "    knowledge_source: str\n",
    "    generation: str\n",
    "\n",
    "# Components\n",
    "config = LocalAgentConfig()\n",
    "llm = ChatOllama(model=config.ollama.model, base_url=config.ollama.base_url, temperature=0)\n",
    "retriever = LocalRetriever()\n",
    "grader = DocumentGrader(llm)\n",
    "\n",
    "# Nodes\n",
    "def retrieve(state):\n",
    "    return {\"documents\": retriever.retrieve_documents(state[\"question\"], k=4)}\n",
    "\n",
    "def grade(state):\n",
    "    relevant, _ = grader.grade_documents(state[\"documents\"], state[\"question\"])\n",
    "    source = \"local\" if len(relevant) >= 2 else (\"combined\" if relevant else \"web\")\n",
    "    return {\"combined_documents\": relevant, \"knowledge_source\": source}\n",
    "\n",
    "def web_search_node(state):\n",
    "    results = web_search(state[\"question\"], 3)\n",
    "    return {\"web_results\": results, \"combined_documents\": state[\"combined_documents\"] + results}\n",
    "\n",
    "def generate_answer(state):\n",
    "    context = \"\\n\\n\".join([d.page_content for d in state[\"combined_documents\"]])\n",
    "    response = llm.invoke(f\"Context: {context}\\n\\nQuestion: {state['question']}\\n\\nAnswer:\")\n",
    "    return {\"generation\": response.content}\n",
    "\n",
    "def route(state) -> str:\n",
    "    return \"generate\" if state[\"knowledge_source\"] == \"local\" else \"web_search\"\n",
    "\n",
    "# Build\n",
    "g = StateGraph(CRAGState)\n",
    "g.add_node(\"retrieve\", retrieve)\n",
    "g.add_node(\"grade\", grade)\n",
    "g.add_node(\"web_search\", web_search_node)\n",
    "g.add_node(\"generate\", generate_answer)\n",
    "g.add_edge(START, \"retrieve\")\n",
    "g.add_edge(\"retrieve\", \"grade\")\n",
    "g.add_conditional_edges(\"grade\", route, {\"generate\": \"generate\", \"web_search\": \"web_search\"})\n",
    "g.add_edge(\"web_search\", \"generate\")\n",
    "g.add_edge(\"generate\", END)\n",
    "\n",
    "crag = g.compile()\n",
    "\n",
    "# Use\n",
    "result = crag.invoke({\"question\": \"What is CRAG?\"})\n",
    "print(f\"Source: {result['knowledge_source']}\")\n",
    "print(f\"Answer: {result['generation'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Key Concepts Recap\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **Knowledge Assessment** | Determine if local docs suffice |\n",
    "| **Web Search Fallback** | External search when needed |\n",
    "| **Source Tracking** | Know where answers came from |\n",
    "| **Conditional Routing** | Dynamic path selection |\n",
    "\n",
    "## CRAG vs Self-RAG\n",
    "\n",
    "| Aspect | Self-RAG | CRAG |\n",
    "|--------|----------|------|\n",
    "| Focus | Quality checking | Knowledge gaps |\n",
    "| Fallback | Retry generation | Web search |\n",
    "| Best for | Accuracy | Coverage |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In [Tutorial 11: Adaptive RAG](11_adaptive_rag.ipynb), you'll learn:\n",
    "- Query classification and routing\n",
    "- Multiple retrieval strategies\n",
    "- Intelligent source selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
