{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 22: Reflection Pattern\n",
    "\n",
    "In this tutorial, you'll learn how to build self-improving agents that iteratively refine their outputs through reflection and critique.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Generate-Critique-Revise loop**: Iterative improvement pattern\n",
    "- **Quality control**: Using critique to guide revisions\n",
    "- **Approval signals**: Explicit stopping conditions\n",
    "- **Multi-criteria reflection**: Structured scoring across dimensions\n",
    "- **Multi-model reflection**: Using different LLMs for generation vs critique\n",
    "\n",
    "By the end, you'll have agents that write, critique, and revise their own work to produce high-quality outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reflection?\n",
    "\n",
    "Reflection is a pattern where an LLM:\n",
    "1. **Generates** an initial draft\n",
    "2. **Critiques** the draft (can be same or different LLM)\n",
    "3. **Revises** based on critique feedback\n",
    "4. **Repeats** until approved or max iterations reached\n",
    "\n",
    "This mirrors how humans improve their work through drafts and revisions.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "START → Generator → Critic → Should Continue?\n",
    "           ↑                        |\n",
    "           |                        |\n",
    "           └────── If Needs ────────┘\n",
    "                   Revision\n",
    "                                    |\n",
    "                                    ├──→ END (if APPROVED)\n",
    "                                    └──→ END (if max iterations)\n",
    "```\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Writing**: Essays, reports, documentation, emails\n",
    "- **Code Generation**: Generate, review, refactor\n",
    "- **Analysis**: Initial assessment, critique, refinement\n",
    "- **Creative Content**: Iterative improvement of creative outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "print(f\"Ollama: {config.ollama.base_url}\")\n",
    "print(f\"Model: {config.ollama.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0.7,  # Some creativity for writing\n",
    ")\n",
    "\n",
    "print(\"LLM configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Reflection Pattern\n",
    "\n",
    "Let's start with the basic reflection pattern using our module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local.patterns.reflection import (\n",
    "    create_reflection_graph,\n",
    "    run_reflection_task,\n",
    ")\n",
    "\n",
    "# Create a basic reflection graph\n",
    "graph = create_reflection_graph(\n",
    "    llm=llm,\n",
    "    max_iterations=3,\n",
    ")\n",
    "\n",
    "print(\"Reflection graph created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not render graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Essay Writing\n",
    "\n",
    "Let's use reflection to write a short essay with iterative improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"Write a brief essay (200-300 words) explaining why LangGraph is useful \n",
    "for building AI agents. Include specific benefits and use cases.\"\"\"\n",
    "\n",
    "print(\"Task:\", task)\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = run_reflection_task(\n",
    "    graph=graph,\n",
    "    task=task,\n",
    "    max_iterations=3,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DRAFT:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"draft\"])\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Iterations: {result['iteration']}\")\n",
    "print(f\"Final Critique: {result['critique'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Professional Email\n",
    "\n",
    "Reflection is excellent for writing professional communications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_task = \"\"\"Write a professional email to a client explaining a 2-week delay \n",
    "in project delivery due to unexpected technical challenges. The email should:\n",
    "- Acknowledge the delay professionally\n",
    "- Explain the challenges briefly\n",
    "- Provide a new realistic timeline\n",
    "- Express commitment to quality\n",
    "- Maintain a positive, confident tone\"\"\"\n",
    "\n",
    "print(\"Task: Professional Email\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = run_reflection_task(\n",
    "    graph=graph,\n",
    "    task=email_task,\n",
    "    max_iterations=3,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EMAIL:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"draft\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Step-by-Step Implementation\n",
    "\n",
    "Let's build a reflection system from scratch to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class ReflectionState(TypedDict):\n",
    "    \"\"\"State for reflection loop.\"\"\"\n",
    "    messages: Annotated[list, add_messages]  # Conversation history\n",
    "    task: str                                 # Original task\n",
    "    draft: str                                # Current draft\n",
    "    critique: str                             # Latest critique\n",
    "    iteration: int                            # Current iteration\n",
    "    max_iterations: int                       # Max iterations allowed\n",
    "\n",
    "print(\"State defined with: messages, task, draft, critique, iteration, max_iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Generator Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "GENERATOR_PROMPT = \"\"\"You are a skilled writer. Your task is to write or revise content.\n",
    "\n",
    "If this is the first draft, write a complete response.\n",
    "If you have received critique, revise your draft to address the feedback.\n",
    "\n",
    "Focus on:\n",
    "- Clarity and conciseness\n",
    "- Accuracy and completeness\n",
    "- Engaging and professional tone\"\"\"\n",
    "\n",
    "def generate_node(state: ReflectionState) -> dict:\n",
    "    \"\"\"Generate or revise the draft.\"\"\"\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "    task = state[\"task\"]\n",
    "    draft = state.get(\"draft\", \"\")\n",
    "    critique = state.get(\"critique\", \"\")\n",
    "    \n",
    "    if iteration == 0:\n",
    "        # First draft\n",
    "        user_msg = f\"Write a response to this task:\\n\\n{task}\"\n",
    "    else:\n",
    "        # Revision based on critique\n",
    "        user_msg = f\"\"\"Revise this draft based on the critique.\n",
    "\n",
    "ORIGINAL TASK: {task}\n",
    "\n",
    "CURRENT DRAFT:\n",
    "{draft}\n",
    "\n",
    "CRITIQUE:\n",
    "{critique}\n",
    "\n",
    "Please provide an improved version:\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=GENERATOR_PROMPT),\n",
    "        HumanMessage(content=user_msg)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    new_draft = response.content\n",
    "    \n",
    "    print(f\"\\n=== Generation (iteration {iteration + 1}) ===\")\n",
    "    print(new_draft[:200] + \"...\" if len(new_draft) > 200 else new_draft)\n",
    "    \n",
    "    return {\n",
    "        \"draft\": new_draft,\n",
    "        \"iteration\": iteration + 1,\n",
    "        \"messages\": [AIMessage(content=f\"Draft {iteration + 1} generated\")]\n",
    "    }\n",
    "\n",
    "print(\"Generator node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Critic Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITIC_PROMPT = \"\"\"You are a thoughtful editor. Review the draft and provide constructive feedback.\n",
    "\n",
    "Evaluate:\n",
    "1. Does it fully address the task?\n",
    "2. Is it clear and well-organized?\n",
    "3. Are there any errors or areas for improvement?\n",
    "4. Is the tone appropriate?\n",
    "\n",
    "If the draft is excellent and needs no changes, respond with exactly: \"APPROVED\"\n",
    "Otherwise, provide specific, actionable feedback for improvement.\"\"\"\n",
    "\n",
    "def critique_node(state: ReflectionState) -> dict:\n",
    "    \"\"\"Critique the current draft.\"\"\"\n",
    "    task = state[\"task\"]\n",
    "    draft = state[\"draft\"]\n",
    "    iteration = state[\"iteration\"]\n",
    "    \n",
    "    user_msg = f\"\"\"Review this draft for the given task.\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "DRAFT:\n",
    "{draft}\n",
    "\n",
    "Provide your critique or \"APPROVED\" if excellent:\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=CRITIC_PROMPT),\n",
    "        HumanMessage(content=user_msg)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    critique = response.content\n",
    "    \n",
    "    print(f\"\\n=== Critique (iteration {iteration}) ===\")\n",
    "    print(critique[:200] + \"...\" if len(critique) > 200 else critique)\n",
    "    \n",
    "    return {\n",
    "        \"critique\": critique,\n",
    "        \"messages\": [AIMessage(content=f\"Critique {iteration} provided\")]\n",
    "    }\n",
    "\n",
    "print(\"Critic node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Routing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def should_continue(state: ReflectionState) -> str:\n",
    "    \"\"\"Decide whether to continue refining or finish.\"\"\"\n",
    "    iteration = state[\"iteration\"]\n",
    "    max_iterations = state[\"max_iterations\"]\n",
    "    critique = state.get(\"critique\", \"\")\n",
    "    \n",
    "    # Stop if approved\n",
    "    if \"APPROVED\" in critique.upper():\n",
    "        print(\"\\n✓ Draft approved!\")\n",
    "        return END\n",
    "    \n",
    "    # Stop if max iterations reached\n",
    "    if iteration >= max_iterations:\n",
    "        print(f\"\\n✓ Max iterations ({max_iterations}) reached\")\n",
    "        return END\n",
    "    \n",
    "    print(f\"\\n→ Continuing to iteration {iteration + 1}\")\n",
    "    return \"generator\"\n",
    "\n",
    "print(\"Routing logic created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Build the reflection graph\n",
    "workflow = StateGraph(ReflectionState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"generator\", generate_node)\n",
    "workflow.add_node(\"critic\", critique_node)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"generator\")       # Start by generating\n",
    "workflow.add_edge(\"generator\", \"critic\")    # Then critique\n",
    "workflow.add_conditional_edges(\n",
    "    \"critic\",\n",
    "    should_continue,\n",
    "    {\"generator\": \"generator\", END: END}\n",
    ")\n",
    "\n",
    "# Compile\n",
    "custom_graph = workflow.compile()\n",
    "\n",
    "print(\"Custom reflection graph compiled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Test the Custom Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a technical explanation task\n",
    "task = \"Explain how neural networks learn through backpropagation in 150 words.\"\n",
    "\n",
    "print(f\"Task: {task}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = custom_graph.invoke({\n",
    "    \"task\": task,\n",
    "    \"messages\": [],\n",
    "    \"draft\": \"\",\n",
    "    \"critique\": \"\",\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\": 3,\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DRAFT:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"draft\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multi-Criteria Reflection\n",
    "\n",
    "Use structured scoring across multiple criteria for more nuanced feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-criteria reflection graph\n",
    "multi_criteria_graph = create_reflection_graph(\n",
    "    llm=llm,\n",
    "    max_iterations=4,\n",
    "    use_multi_criteria=True,\n",
    "    approval_threshold=8,  # Require score of 8/10 or higher\n",
    ")\n",
    "\n",
    "print(\"Multi-criteria reflection graph created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a complex writing task\n",
    "task = \"\"\"Write a technical blog post introduction (200 words) about the benefits \n",
    "of using LangGraph for AI agent development. The introduction should:\n",
    "- Hook the reader with a compelling opening\n",
    "- Clearly state the problem being solved\n",
    "- Preview the main benefits to be discussed\n",
    "- Use a professional but accessible tone\"\"\"\n",
    "\n",
    "print(\"Task: Technical Blog Post Introduction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = run_reflection_task(\n",
    "    graph=multi_criteria_graph,\n",
    "    task=task,\n",
    "    max_iterations=4,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL INTRODUCTION:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"draft\"])\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL CRITIQUE WITH SCORES:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"critique\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Model Reflection (Advanced)\n",
    "\n",
    "Use different models for generation and critique:\n",
    "- Fast model for generation (e.g., llama3.2:3b)\n",
    "- Stronger model for critique (e.g., llama3.1:70b)\n",
    "\n",
    "This can improve critique quality while keeping generation efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local.patterns.reflection import create_multi_model_reflection_graph\n",
    "\n",
    "# Use the same model for both in this example\n",
    "# In production, you might use:\n",
    "# generator_llm = ChatOllama(model=\"llama3.2:3b\")  # Fast for drafts\n",
    "# critic_llm = ChatOllama(model=\"llama3.1:70b\")     # Thorough for critique\n",
    "\n",
    "generator_llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0.8,  # More creative for generation\n",
    ")\n",
    "\n",
    "critic_llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0.3,  # More focused for critique\n",
    ")\n",
    "\n",
    "multi_model_graph = create_multi_model_reflection_graph(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    max_iterations=3,\n",
    ")\n",
    "\n",
    "print(\"Multi-model reflection graph created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-model reflection\n",
    "task = \"\"\"Write a creative product description (150 words) for a smart home device \n",
    "that uses AI to learn your daily routines and automatically adjust lighting, \n",
    "temperature, and music preferences.\"\"\"\n",
    "\n",
    "print(\"Task: Creative Product Description\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = run_reflection_task(\n",
    "    graph=multi_model_graph,\n",
    "    task=task,\n",
    "    max_iterations=3,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL PRODUCT DESCRIPTION:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"draft\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts Recap\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Generator** | Creates initial drafts and revisions |\n",
    "| **Critic** | Evaluates drafts and provides feedback |\n",
    "| **Approval Signal** | Explicit \"APPROVED\" signal to stop refining |\n",
    "| **Iteration Limit** | Safety mechanism to prevent infinite loops |\n",
    "| **Multi-Criteria** | Structured scoring across multiple dimensions |\n",
    "| **Multi-Model** | Different LLMs for generation vs critique |\n",
    "\n",
    "## Reflection vs Reflexion\n",
    "\n",
    "| Pattern | Focus | Memory | Use Case |\n",
    "|---------|-------|--------|----------|\n",
    "| **Reflection** | Single output improvement | Current draft only | Writing, content creation |\n",
    "| **Reflexion** | Learning across attempts | Episodic memory | Complex problem-solving |\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Set reasonable iteration limits** (3-5 typically sufficient)\n",
    "2. **Use clear approval signals** (\"APPROVED\" in critique)\n",
    "3. **Provide specific critique prompts** for better feedback\n",
    "4. **Consider multi-model** for quality vs speed tradeoffs\n",
    "5. **Use multi-criteria** for complex quality requirements\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Tutorial 23: Reflexion Pattern**, you'll learn:\n",
    "- Learning from multiple attempts\n",
    "- Episodic memory across trials\n",
    "- Tool-augmented reflection\n",
    "- When to use Reflexion vs Reflection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
