{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 20: Multi-Agent Evaluation\n",
    "\n",
    "In this tutorial, you'll learn how to evaluate agents using **simulated users** and **automated evaluators**. This pattern enables systematic testing of agents through realistic conversations.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Creating simulated user agents with personas and goals\n",
    "- Building automated evaluator agents that score conversations\n",
    "- Orchestrating evaluation sessions with metrics collection\n",
    "- Aggregating scores across multiple evaluation runs\n",
    "- Using evaluation to improve agent quality\n",
    "\n",
    "By the end, you'll have a working evaluation system that can test any agent through simulated conversations and provide objective quality scores.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Tutorials 14-16 (Multi-Agent Patterns)\n",
    "- Understanding of multi-agent collaboration\n",
    "- Ollama running with a capable model (llama3.1:8b or larger recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Multi-Agent Evaluation?\n",
    "\n",
    "Testing agents is challenging because:\n",
    "\n",
    "1. **Manual testing doesn't scale**: Testing every change manually is time-consuming\n",
    "2. **Need consistent metrics**: Subjective assessment varies between testers\n",
    "3. **Edge cases matter**: Agents should handle various user behaviors\n",
    "4. **Quality is multi-dimensional**: Helpfulness, accuracy, empathy all matter\n",
    "\n",
    "**Multi-agent evaluation** solves this by:\n",
    "- Using **simulated users** to generate realistic test conversations\n",
    "- Using **evaluator agents** to score conversations objectively\n",
    "- Running **multiple sessions** to cover different scenarios\n",
    "- **Aggregating metrics** for overall quality assessment\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                  Evaluation Session                      │\n",
    "│                                                           │\n",
    "│  ┌──────────┐       ┌──────────┐       ┌──────────┐    │\n",
    "│  │  Agent   │◄─────►│Simulated │       │Evaluator │    │\n",
    "│  │ Under    │       │  User    │──────►│  Agent   │    │\n",
    "│  │  Test    │       │  Agent   │       │          │    │\n",
    "│  └──────────┘       └──────────┘       └────┬─────┘    │\n",
    "│                                              │           │\n",
    "│                                              ▼           │\n",
    "│                                        ┌──────────┐     │\n",
    "│                                        │  Scores  │     │\n",
    "│                                        │  Metrics │     │\n",
    "│                                        └──────────┘     │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our setup\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "print(f\"Ollama URL: {config.ollama.base_url}\")\n",
    "print(f\"Model: {config.ollama.model}\")\n",
    "print(\"Setup verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Evaluation State\n",
    "\n",
    "Our state tracks:\n",
    "- The conversation between agent and simulated user\n",
    "- Scores from the evaluator agent\n",
    "- Turn count and completion status\n",
    "- Final aggregated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "\n",
    "\n",
    "class EvaluationState(TypedDict):\n",
    "    \"\"\"State for agent evaluation sessions.\"\"\"\n",
    "    \n",
    "    # Conversation messages\n",
    "    messages: Annotated[list, add_messages]\n",
    "    \n",
    "    # Formatted conversation for evaluator\n",
    "    conversation: str\n",
    "    \n",
    "    # Accumulated scores - uses operator.add to append\n",
    "    evaluator_scores: Annotated[list[dict], operator.add]\n",
    "    \n",
    "    # Turn tracking\n",
    "    turn_count: int\n",
    "    max_turns: int\n",
    "    \n",
    "    # Session status\n",
    "    session_complete: bool\n",
    "    \n",
    "    # Final metrics\n",
    "    final_metrics: dict[str, float]\n",
    "\n",
    "\n",
    "print(\"Evaluation state defined!\")\n",
    "print(\"\\nKey fields:\")\n",
    "print(\"- evaluator_scores: Accumulates scores from evaluator agent\")\n",
    "print(\"- final_metrics: Aggregated scores (helpfulness, accuracy, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure the Simulated User\n",
    "\n",
    "The simulated user acts as a realistic test user with:\n",
    "- A specific persona and situation\n",
    "- Goals to achieve in the conversation\n",
    "- A behavioral style (friendly, impatient, confused, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class SimulatedUser(BaseModel):\n",
    "    \"\"\"Configuration for simulated user behavior.\"\"\"\n",
    "    \n",
    "    persona: str = Field(\n",
    "        description=\"User's background and situation\"\n",
    "    )\n",
    "    goals: list[str] = Field(\n",
    "        description=\"What the user wants to accomplish\"\n",
    "    )\n",
    "    behavior: Literal[\"friendly\", \"impatient\", \"confused\", \"technical\", \"casual\"] = Field(\n",
    "        default=\"friendly\",\n",
    "        description=\"Communication style\"\n",
    "    )\n",
    "    initial_message: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"First message (auto-generated if None)\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Example: Customer service scenario\n",
    "frustrated_customer = SimulatedUser(\n",
    "    persona=\"Customer who received a damaged product and wants a refund. Has been waiting 2 weeks for resolution.\",\n",
    "    goals=[\n",
    "        \"Get full refund processed\",\n",
    "        \"Express dissatisfaction with shipping delay\",\n",
    "        \"Ensure future orders won't have same issue\"\n",
    "    ],\n",
    "    behavior=\"impatient\",\n",
    "    initial_message=\"Hi, I'm really frustrated. My order arrived damaged 2 weeks ago and I still haven't gotten a refund!\"\n",
    ")\n",
    "\n",
    "print(\"Simulated user configured!\")\n",
    "print(f\"\\nPersona: {frustrated_customer.persona}\")\n",
    "print(f\"Goals: {frustrated_customer.goals}\")\n",
    "print(f\"Behavior: {frustrated_customer.behavior}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Evaluation Criteria\n",
    "\n",
    "The evaluator scores conversations on multiple dimensions using structured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationCriteria(BaseModel):\n",
    "    \"\"\"Structured scores for conversation evaluation.\"\"\"\n",
    "    \n",
    "    helpfulness: int = Field(\n",
    "        ge=1, le=5,\n",
    "        description=\"How helpful were the responses? (1-5)\"\n",
    "    )\n",
    "    accuracy: int = Field(\n",
    "        ge=1, le=5,\n",
    "        description=\"How accurate was the information? (1-5)\"\n",
    "    )\n",
    "    empathy: int = Field(\n",
    "        ge=1, le=5,\n",
    "        description=\"How empathetic was the agent? (1-5)\"\n",
    "    )\n",
    "    efficiency: int = Field(\n",
    "        ge=1, le=5,\n",
    "        description=\"How efficient was the conversation? (1-5)\"\n",
    "    )\n",
    "    goal_completion: int = Field(\n",
    "        ge=0, le=1,\n",
    "        description=\"Were user's goals achieved? (0=no, 1=yes)\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Explanation of the scores\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Evaluation criteria defined!\")\n",
    "print(\"\\nScoring dimensions:\")\n",
    "print(\"- Helpfulness (1-5)\")\n",
    "print(\"- Accuracy (1-5)\")\n",
    "print(\"- Empathy (1-5)\")\n",
    "print(\"- Efficiency (1-5)\")\n",
    "print(\"- Goal Completion (0 or 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the Simulated User Node\n",
    "\n",
    "This node generates user messages based on persona and conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "def create_simulated_user_node(llm, user_config: SimulatedUser):\n",
    "    \"\"\"Create simulated user agent.\"\"\"\n",
    "    \n",
    "    system_prompt = f\"\"\"You are simulating a user in a conversation.\n",
    "\n",
    "**Your Persona:**\n",
    "{user_config.persona}\n",
    "\n",
    "**Your Goals:**\n",
    "{chr(10).join(f\"- {goal}\" for goal in user_config.goals)}\n",
    "\n",
    "**Your Behavior:**\n",
    "You are {user_config.behavior}.\n",
    "\n",
    "**Instructions:**\n",
    "- Stay in character throughout the conversation\n",
    "- Work towards your goals naturally\n",
    "- Keep responses concise (1-3 sentences)\n",
    "- Express satisfaction if goals are met\n",
    "- Don't mention that you're simulating anything\"\"\"\n",
    "    \n",
    "    def simulated_user(state: EvaluationState) -> dict:\n",
    "        \"\"\"Generate user message.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        turn_count = state.get(\"turn_count\", 0)\n",
    "        \n",
    "        # Use initial message for first turn\n",
    "        if turn_count == 0 and user_config.initial_message:\n",
    "            user_message = user_config.initial_message\n",
    "        else:\n",
    "            # Generate response based on conversation\n",
    "            prompt_messages = [\n",
    "                SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=f\"\"\"Based on the conversation, provide your next message.\n",
    "\n",
    "Recent conversation:\n",
    "{chr(10).join(f\"{m.type}: {m.content}\" for m in messages[-6:] if messages)}\n",
    "\n",
    "Your next message:\"\"\")\n",
    "            ]\n",
    "            \n",
    "            response = llm.invoke(prompt_messages)\n",
    "            user_message = response.content\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [HumanMessage(content=user_message)],\n",
    "            \"turn_count\": turn_count + 1,\n",
    "        }\n",
    "    \n",
    "    return simulated_user\n",
    "\n",
    "\n",
    "print(\"Simulated user node creator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create the Evaluator Node\n",
    "\n",
    "This node reviews conversations and provides structured scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluator_node(llm):\n",
    "    \"\"\"Create evaluator agent.\"\"\"\n",
    "    \n",
    "    # Use structured output for reliable scoring\n",
    "    structured_llm = llm.with_structured_output(EvaluationCriteria)\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert evaluator of customer service conversations.\n",
    "\n",
    "Score conversations objectively on:\n",
    "- Helpfulness: How helpful were responses? (1-5)\n",
    "- Accuracy: How accurate was the information? (1-5)\n",
    "- Empathy: How empathetic was the agent? (1-5)\n",
    "- Efficiency: How concise and direct? (1-5)\n",
    "- Goal Completion: Were user goals achieved? (0=no, 1=yes)\n",
    "\n",
    "Be fair and consider full context.\"\"\"\n",
    "    \n",
    "    def evaluator(state: EvaluationState) -> dict:\n",
    "        \"\"\"Evaluate conversation and provide scores.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        \n",
    "        # Format conversation\n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{'User' if isinstance(m, HumanMessage) else 'Agent'}: {m.content}\"\n",
    "            for m in messages\n",
    "        ])\n",
    "        \n",
    "        eval_messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=f\"\"\"Evaluate this conversation:\n",
    "\n",
    "{conversation_text}\n",
    "\n",
    "Provide scores for all criteria.\"\"\")\n",
    "        ]\n",
    "        \n",
    "        criteria = structured_llm.invoke(eval_messages)\n",
    "        \n",
    "        scores = {\n",
    "            \"helpfulness\": criteria.helpfulness,\n",
    "            \"accuracy\": criteria.accuracy,\n",
    "            \"empathy\": criteria.empathy,\n",
    "            \"efficiency\": criteria.efficiency,\n",
    "            \"goal_completion\": criteria.goal_completion,\n",
    "            \"reasoning\": criteria.reasoning,\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"evaluator_scores\": [scores],\n",
    "            \"conversation\": conversation_text,\n",
    "        }\n",
    "    \n",
    "    return evaluator\n",
    "\n",
    "\n",
    "print(\"Evaluator node creator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create the Agent Under Test\n",
    "\n",
    "Let's build a simple customer service agent to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "\n",
    "def create_customer_service_agent(llm):\n",
    "    \"\"\"Create a customer service agent to test.\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a helpful customer service agent.\n",
    "\n",
    "Your responsibilities:\n",
    "- Listen to customer concerns with empathy\n",
    "- Provide clear, accurate information\n",
    "- Offer solutions to resolve issues\n",
    "- Be professional and courteous\n",
    "\n",
    "For refund requests:\n",
    "- Apologize for the inconvenience\n",
    "- Confirm you'll process the refund\n",
    "- Provide a timeline (3-5 business days)\n",
    "- Offer to help with anything else\"\"\"\n",
    "    \n",
    "    def agent(state: EvaluationState) -> dict:\n",
    "        \"\"\"Respond to customer message.\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        \n",
    "        # Build prompt with conversation history\n",
    "        prompt_messages = [SystemMessage(content=system_prompt)]\n",
    "        prompt_messages.extend(messages)\n",
    "        \n",
    "        response = llm.invoke(prompt_messages)\n",
    "        \n",
    "        return {\"messages\": [AIMessage(content=response.content)]}\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "customer_service_agent = create_customer_service_agent(llm)\n",
    "print(\"Customer service agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Helper Nodes\n",
    "\n",
    "We need nodes to check completion and finalize metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_check_completion_node():\n",
    "    \"\"\"Check if evaluation session should end.\"\"\"\n",
    "    \n",
    "    def check_completion(state: EvaluationState) -> dict:\n",
    "        turn_count = state.get(\"turn_count\", 0)\n",
    "        max_turns = state.get(\"max_turns\", 10)\n",
    "        \n",
    "        # Check max turns\n",
    "        if turn_count >= max_turns:\n",
    "            return {\"session_complete\": True}\n",
    "        \n",
    "        # Check if user seems satisfied\n",
    "        messages = state.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_message = messages[-1].content.lower()\n",
    "            if any(phrase in last_message for phrase in [\n",
    "                \"thank you\", \"thanks\", \"goodbye\", \"bye\",\n",
    "                \"that's all\", \"all set\", \"perfect\"\n",
    "            ]):\n",
    "                return {\"session_complete\": True}\n",
    "        \n",
    "        return {\"session_complete\": False}\n",
    "    \n",
    "    return check_completion\n",
    "\n",
    "\n",
    "def aggregate_scores(scores: list[dict]) -> dict[str, float]:\n",
    "    \"\"\"Aggregate evaluator scores.\"\"\"\n",
    "    if not scores:\n",
    "        return {\n",
    "            \"helpfulness_avg\": 0.0,\n",
    "            \"accuracy_avg\": 0.0,\n",
    "            \"empathy_avg\": 0.0,\n",
    "            \"efficiency_avg\": 0.0,\n",
    "            \"goal_completion_rate\": 0.0,\n",
    "            \"num_scores\": 0,\n",
    "        }\n",
    "    \n",
    "    num_scores = len(scores)\n",
    "    \n",
    "    return {\n",
    "        \"helpfulness_avg\": sum(s.get(\"helpfulness\", 0) for s in scores) / num_scores,\n",
    "        \"accuracy_avg\": sum(s.get(\"accuracy\", 0) for s in scores) / num_scores,\n",
    "        \"empathy_avg\": sum(s.get(\"empathy\", 0) for s in scores) / num_scores,\n",
    "        \"efficiency_avg\": sum(s.get(\"efficiency\", 0) for s in scores) / num_scores,\n",
    "        \"goal_completion_rate\": sum(s.get(\"goal_completion\", 0) for s in scores) / num_scores,\n",
    "        \"num_scores\": num_scores,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_finalize_node():\n",
    "    \"\"\"Finalize evaluation metrics.\"\"\"\n",
    "    \n",
    "    def finalize(state: EvaluationState) -> dict:\n",
    "        scores = state.get(\"evaluator_scores\", [])\n",
    "        metrics = aggregate_scores(scores)\n",
    "        return {\"final_metrics\": metrics}\n",
    "    \n",
    "    return finalize\n",
    "\n",
    "\n",
    "print(\"Helper nodes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Build the Evaluation Graph\n",
    "\n",
    "Now we assemble all pieces into the evaluation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "def create_evaluation_graph(\n",
    "    llm,\n",
    "    agent_node,\n",
    "    user_config: SimulatedUser,\n",
    "    evaluate_every_n_turns: int = 2,\n",
    "):\n",
    "    \"\"\"Build evaluation graph.\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(EvaluationState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"simulated_user\", create_simulated_user_node(llm, user_config))\n",
    "    workflow.add_node(\"agent\", agent_node)\n",
    "    workflow.add_node(\"evaluator\", create_evaluator_node(llm))\n",
    "    workflow.add_node(\"check_completion\", create_check_completion_node())\n",
    "    workflow.add_node(\"finalize\", create_finalize_node())\n",
    "    \n",
    "    # Entry: simulated user starts\n",
    "    workflow.add_edge(START, \"simulated_user\")\n",
    "    \n",
    "    # User -> Agent\n",
    "    workflow.add_edge(\"simulated_user\", \"agent\")\n",
    "    \n",
    "    # Agent -> Check completion\n",
    "    workflow.add_edge(\"agent\", \"check_completion\")\n",
    "    \n",
    "    # Routing after check\n",
    "    def route_after_check(state: EvaluationState) -> str:\n",
    "        if state.get(\"session_complete\", False):\n",
    "            return \"finalize\"\n",
    "        \n",
    "        # Evaluate periodically\n",
    "        turn_count = state.get(\"turn_count\", 0)\n",
    "        if turn_count % evaluate_every_n_turns == 0 and turn_count > 0:\n",
    "            return \"evaluator\"\n",
    "        \n",
    "        return \"simulated_user\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"check_completion\",\n",
    "        route_after_check,\n",
    "        {\n",
    "            \"simulated_user\": \"simulated_user\",\n",
    "            \"evaluator\": \"evaluator\",\n",
    "            \"finalize\": \"finalize\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Evaluator loops back\n",
    "    workflow.add_edge(\"evaluator\", \"simulated_user\")\n",
    "    \n",
    "    # Finalize ends\n",
    "    workflow.add_edge(\"finalize\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "eval_graph = create_evaluation_graph(\n",
    "    llm,\n",
    "    customer_service_agent,\n",
    "    frustrated_customer,\n",
    "    evaluate_every_n_turns=2\n",
    ")\n",
    "\n",
    "print(\"Evaluation graph compiled!\")\n",
    "print(\"\\nGraph flow:\")\n",
    "print(\"  START -> simulated_user\")\n",
    "print(\"  simulated_user -> agent\")\n",
    "print(\"  agent -> check_completion\")\n",
    "print(\"  check_completion -> [simulated_user | evaluator | finalize]\")\n",
    "print(\"  evaluator -> simulated_user\")\n",
    "print(\"  finalize -> END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Run an Evaluation Session\n",
    "\n",
    "Let's test our customer service agent with the frustrated customer scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_session(graph, max_turns: int = 10):\n",
    "    \"\"\"Run complete evaluation session.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING EVALUATION SESSION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_state: EvaluationState = {\n",
    "        \"messages\": [],\n",
    "        \"conversation\": \"\",\n",
    "        \"evaluator_scores\": [],\n",
    "        \"turn_count\": 0,\n",
    "        \"max_turns\": max_turns,\n",
    "        \"session_complete\": False,\n",
    "        \"final_metrics\": {},\n",
    "    }\n",
    "    \n",
    "    result = graph.invoke(initial_state)\n",
    "    \n",
    "    # Display conversation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONVERSATION\")\n",
    "    print(\"=\"*60)\n",
    "    for msg in result[\"messages\"]:\n",
    "        role = \"User\" if isinstance(msg, HumanMessage) else \"Agent\"\n",
    "        print(f\"\\n{role}: {msg.content}\")\n",
    "    \n",
    "    # Display scores\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION SCORES\")\n",
    "    print(\"=\"*60)\n",
    "    for i, score in enumerate(result[\"evaluator_scores\"], 1):\n",
    "        print(f\"\\nEvaluation {i}:\")\n",
    "        print(f\"  Helpfulness: {score['helpfulness']}/5\")\n",
    "        print(f\"  Accuracy: {score['accuracy']}/5\")\n",
    "        print(f\"  Empathy: {score['empathy']}/5\")\n",
    "        print(f\"  Efficiency: {score['efficiency']}/5\")\n",
    "        print(f\"  Goal Completion: {score['goal_completion']}\")\n",
    "        print(f\"  Reasoning: {score['reasoning']}\")\n",
    "    \n",
    "    # Display final metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    metrics = result[\"final_metrics\"]\n",
    "    print(f\"Helpfulness:      {metrics['helpfulness_avg']:.2f}/5.00\")\n",
    "    print(f\"Accuracy:         {metrics['accuracy_avg']:.2f}/5.00\")\n",
    "    print(f\"Empathy:          {metrics['empathy_avg']:.2f}/5.00\")\n",
    "    print(f\"Efficiency:       {metrics['efficiency_avg']:.2f}/5.00\")\n",
    "    print(f\"Goal Completion:  {metrics['goal_completion_rate']:.0%}\")\n",
    "    print(f\"\\nTotal Turns: {result['turn_count']}\")\n",
    "    print(f\"Evaluations: {metrics['num_scores']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "result = run_evaluation_session(eval_graph, max_turns=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Test Different User Scenarios\n",
    "\n",
    "Let's evaluate with different simulated user types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Friendly customer with simple question\n",
    "friendly_customer = SimulatedUser(\n",
    "    persona=\"Customer interested in learning about return policy before making a purchase.\",\n",
    "    goals=[\"Understand return policy\", \"Clarify timeframe for returns\"],\n",
    "    behavior=\"friendly\",\n",
    "    initial_message=\"Hi! I'm thinking of ordering something, but I'd like to know about your return policy first.\"\n",
    ")\n",
    "\n",
    "friendly_graph = create_evaluation_graph(\n",
    "    llm,\n",
    "    customer_service_agent,\n",
    "    friendly_customer,\n",
    ")\n",
    "\n",
    "result2 = run_evaluation_session(friendly_graph, max_turns=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Confused customer\n",
    "confused_customer = SimulatedUser(\n",
    "    persona=\"Elderly customer who is not tech-savvy and having trouble tracking an order online.\",\n",
    "    goals=[\"Find out where order is\", \"Get help with tracking\"],\n",
    "    behavior=\"confused\",\n",
    "    initial_message=\"Hello, I'm trying to find my order but I don't understand this website. Can you help me?\"\n",
    ")\n",
    "\n",
    "confused_graph = create_evaluation_graph(\n",
    "    llm,\n",
    "    customer_service_agent,\n",
    "    confused_customer,\n",
    ")\n",
    "\n",
    "result3 = run_evaluation_session(confused_graph, max_turns=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Using the Built-in Module\n",
    "\n",
    "The `langgraph_ollama_local.patterns` module provides ready-to-use evaluation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local.patterns.evaluation import (\n",
    "    SimulatedUser,\n",
    "    create_evaluation_graph,\n",
    "    run_evaluation_session,\n",
    "    run_multiple_evaluations,\n",
    ")\n",
    "\n",
    "# Use the module's implementation\n",
    "user_config = SimulatedUser(\n",
    "    persona=\"Customer with billing question\",\n",
    "    goals=[\"Understand unexpected charge\", \"Get charge removed if incorrect\"],\n",
    "    behavior=\"friendly\",\n",
    ")\n",
    "\n",
    "module_graph = create_evaluation_graph(\n",
    "    llm,\n",
    "    customer_service_agent,\n",
    "    user_config\n",
    ")\n",
    "\n",
    "# Run single session\n",
    "result = run_evaluation_session(module_graph, max_turns=5)\n",
    "\n",
    "print(\"\\nFinal Metrics:\")\n",
    "for metric, value in result[\"final_metrics\"].items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Run Multiple Evaluations\n",
    "\n",
    "For robust testing, run multiple evaluation sessions and aggregate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3 evaluation sessions\n",
    "multi_results = run_multiple_evaluations(\n",
    "    module_graph,\n",
    "    num_sessions=3,\n",
    "    max_turns=6\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AGGREGATE METRICS (3 SESSIONS)\")\n",
    "print(\"=\"*60)\n",
    "metrics = multi_results[\"aggregate_metrics\"]\n",
    "print(f\"Helpfulness:      {metrics['helpfulness_avg']:.2f}/5.00\")\n",
    "print(f\"Accuracy:         {metrics['accuracy_avg']:.2f}/5.00\")\n",
    "print(f\"Empathy:          {metrics['empathy_avg']:.2f}/5.00\")\n",
    "print(f\"Efficiency:       {metrics['efficiency_avg']:.2f}/5.00\")\n",
    "print(f\"Goal Completion:  {metrics['goal_completion_rate']:.0%}\")\n",
    "print(f\"\\nSessions Evaluated: {metrics['num_sessions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Code\n",
    "\n",
    "Here's the complete implementation in one cell for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Complete Multi-Agent Evaluation Implementation ===\n",
    "\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "\n",
    "# === State ===\n",
    "class EvaluationState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    conversation: str\n",
    "    evaluator_scores: Annotated[list[dict], operator.add]\n",
    "    turn_count: int\n",
    "    max_turns: int\n",
    "    session_complete: bool\n",
    "    final_metrics: dict[str, float]\n",
    "\n",
    "\n",
    "# === Config ===\n",
    "class SimulatedUser(BaseModel):\n",
    "    persona: str\n",
    "    goals: list[str]\n",
    "    behavior: Literal[\"friendly\", \"impatient\", \"confused\", \"technical\", \"casual\"] = \"friendly\"\n",
    "    initial_message: str | None = None\n",
    "\n",
    "\n",
    "class EvaluationCriteria(BaseModel):\n",
    "    helpfulness: int = Field(ge=1, le=5)\n",
    "    accuracy: int = Field(ge=1, le=5)\n",
    "    empathy: int = Field(ge=1, le=5)\n",
    "    efficiency: int = Field(ge=1, le=5)\n",
    "    goal_completion: int = Field(ge=0, le=1)\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "# === Quick Example ===\n",
    "def quick_evaluation_example():\n",
    "    config = LocalAgentConfig()\n",
    "    llm = ChatOllama(model=config.ollama.model, base_url=config.ollama.base_url)\n",
    "    \n",
    "    # Simple agent\n",
    "    def agent(state):\n",
    "        return {\"messages\": [AIMessage(content=\"I'm here to help! How can I assist you?\")]}\n",
    "    \n",
    "    # Simulated user\n",
    "    user = SimulatedUser(\n",
    "        persona=\"Customer with question\",\n",
    "        goals=[\"Get help\"],\n",
    "        behavior=\"friendly\"\n",
    "    )\n",
    "    \n",
    "    from langgraph_ollama_local.patterns.evaluation import (\n",
    "        create_evaluation_graph,\n",
    "        run_evaluation_session,\n",
    "    )\n",
    "    \n",
    "    graph = create_evaluation_graph(llm, agent, user)\n",
    "    result = run_evaluation_session(graph, max_turns=4)\n",
    "    \n",
    "    return result[\"final_metrics\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    metrics = quick_evaluation_example()\n",
    "    print(f\"Helpfulness: {metrics['helpfulness_avg']:.2f}/5.00\")\n",
    "    print(f\"Empathy: {metrics['empathy_avg']:.2f}/5.00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Simulated User** | Agent that mimics real user with persona and goals |\n",
    "| **Evaluator Agent** | Agent that scores conversations objectively |\n",
    "| **Evaluation Session** | Full conversation + periodic scoring |\n",
    "| **Structured Scoring** | Pydantic models ensure consistent evaluation |\n",
    "| **Metrics Aggregation** | Combine scores into summary statistics |\n",
    "| **Multiple Runs** | Test with different scenarios for robustness |\n",
    "| **Periodic Evaluation** | Score every N turns to track quality over time |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Diverse scenarios**: Test with multiple user personas and behaviors\n",
    "2. **Clear goals**: Define specific, measurable objectives for simulated users\n",
    "3. **Multiple sessions**: Run 3-5 evaluations per scenario for reliability\n",
    "4. **Objective criteria**: Use consistent scoring dimensions across tests\n",
    "5. **Track changes**: Compare metrics before/after agent improvements\n",
    "6. **Edge cases**: Test impatient, confused, and difficult user behaviors\n",
    "7. **Reasonable limits**: Set max_turns to prevent endless conversations\n",
    "\n",
    "## What's Next\n",
    "\n",
    "Congratulations! You've completed the Multi-Agent Patterns series. You now know:\n",
    "- Multi-agent collaboration with supervisors (Tutorial 14)\n",
    "- Hierarchical team structures (Tutorial 15)\n",
    "- Subgraph composition patterns (Tutorial 16)\n",
    "- Automated agent evaluation (Tutorial 20)\n",
    "\n",
    "Continue exploring:\n",
    "- Combine evaluation with other patterns for quality assurance\n",
    "- Build custom evaluator agents for domain-specific metrics\n",
    "- Create test suites with diverse simulated users\n",
    "- Use evaluation to guide agent development and iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
