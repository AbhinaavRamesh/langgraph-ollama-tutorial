{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 09: Self-RAG (Self-Reflective RAG)\n",
    "\n",
    "In this tutorial, you'll build a Self-RAG system that **grades its own work** - checking document relevance, detecting hallucinations, and ensuring answer quality.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Document Grading**: Filter irrelevant retrieved documents\n",
    "- **Hallucination Detection**: Check if answers are grounded in facts\n",
    "- **Answer Grading**: Verify the answer addresses the question\n",
    "- **Retry Logic**: Re-generate when quality checks fail\n",
    "\n",
    "By the end, you'll have a RAG system that self-corrects for higher quality outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Why Self-RAG?\n",
    "\n",
    "Basic RAG has blind spots:\n",
    "- Retrieved documents might be irrelevant\n",
    "- LLM might hallucinate despite having context\n",
    "- Answer might not address the actual question\n",
    "\n",
    "**Self-RAG** adds reflection steps:\n",
    "\n",
    "```\n",
    "Retrieve → Grade Docs → Generate → Check Hallucination → Check Answer → Return/Retry\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure you've run Tutorial 08 to index documents, or run:\n",
    "```python\n",
    "from langgraph_ollama_local.rag import DocumentIndexer\n",
    "indexer = DocumentIndexer()\n",
    "indexer.index_directory(\"sources/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f\"Using model: {config.ollama.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 1: Define the State\n",
    "\n",
    "Self-RAG needs to track more information than basic RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class SelfRAGState(TypedDict):\n",
    "    \"\"\"State for Self-RAG pipeline.\"\"\"\n",
    "    question: str                      # User's question\n",
    "    documents: List[Document]          # Retrieved documents\n",
    "    filtered_documents: List[Document] # Relevant documents only\n",
    "    generation: str                    # Generated answer\n",
    "    retry_count: int                   # Number of retries\n",
    "    max_retries: int                   # Maximum retries allowed\n",
    "\n",
    "print(\"State schema defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Create the Graders\n",
    "\n",
    "We'll create three graders using the LLM:\n",
    "1. **Document Grader**: Is this document relevant to the question?\n",
    "2. **Hallucination Grader**: Is the answer grounded in the documents?\n",
    "3. **Answer Grader**: Does the answer address the question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local.rag import DocumentGrader, HallucinationGrader, AnswerGrader\n",
    "\n",
    "# Create graders\n",
    "doc_grader = DocumentGrader(llm)\n",
    "hallucination_grader = HallucinationGrader(llm)\n",
    "answer_grader = AnswerGrader(llm)\n",
    "\n",
    "print(\"Graders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the document grader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "test_doc = Document(\n",
    "    page_content=\"Self-RAG is a framework that enhances language models with self-reflection capabilities. It retrieves documents on-demand and uses reflection tokens to grade its own outputs.\"\n",
    ")\n",
    "\n",
    "relevant_q = \"What is Self-RAG?\"\n",
    "irrelevant_q = \"What is the weather today?\"\n",
    "\n",
    "print(f\"Document: {test_doc.page_content[:100]}...\")\n",
    "print(f\"\\nQuestion 1: '{relevant_q}'\")\n",
    "print(f\"Relevant: {doc_grader.grade(test_doc, relevant_q)}\")\n",
    "print(f\"\\nQuestion 2: '{irrelevant_q}'\")\n",
    "print(f\"Relevant: {doc_grader.grade(test_doc, irrelevant_q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 3: Create the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local.rag import LocalRetriever\n",
    "\n",
    "retriever = LocalRetriever()\n",
    "\n",
    "# Test retrieval\n",
    "test_results = retriever.retrieve(\"What is Self-RAG?\", k=2)\n",
    "print(f\"Retrieved {len(test_results)} documents\")\n",
    "for doc, score in test_results:\n",
    "    print(f\"  Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4: Define Node Functions\n",
    "\n",
    "Now we'll define the nodes for our Self-RAG graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: SelfRAGState) -> dict:\n",
    "    \"\"\"Retrieve documents for the question.\"\"\"\n",
    "    print(f\"--- RETRIEVE ---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.retrieve_documents(question, k=5)\n",
    "    print(f\"Retrieved {len(docs)} documents\")\n",
    "    return {\n",
    "        \"documents\": docs,\n",
    "        \"retry_count\": state.get(\"retry_count\", 0),\n",
    "        \"max_retries\": state.get(\"max_retries\", 3),\n",
    "    }\n",
    "\n",
    "print(\"Retrieve node defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state: SelfRAGState) -> dict:\n",
    "    \"\"\"Grade documents for relevance.\"\"\"\n",
    "    print(f\"--- GRADE DOCUMENTS ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Grade each document\n",
    "    relevant, irrelevant = doc_grader.grade_documents(documents, question)\n",
    "    \n",
    "    print(f\"Relevant: {len(relevant)}, Irrelevant: {len(irrelevant)}\")\n",
    "    return {\"filtered_documents\": relevant}\n",
    "\n",
    "print(\"Grade documents node defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG generation prompt\n",
    "GENERATE_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "Use ONLY the following context to answer the question.\n",
    "If you cannot answer from the context, say \"I cannot answer this from the provided documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "def generate(state: SelfRAGState) -> dict:\n",
    "    \"\"\"Generate answer using filtered documents.\"\"\"\n",
    "    print(f\"--- GENERATE ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"filtered_documents\"]\n",
    "    \n",
    "    if not documents:\n",
    "        return {\"generation\": \"I could not find any relevant documents to answer this question.\"}\n",
    "    \n",
    "    # Format context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # Generate\n",
    "    messages = GENERATE_PROMPT.format_messages(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(f\"Generated {len(response.content)} characters\")\n",
    "    return {\"generation\": response.content}\n",
    "\n",
    "print(\"Generate node defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hallucination(state: SelfRAGState) -> dict:\n",
    "    \"\"\"Check if generation is grounded in documents.\"\"\"\n",
    "    print(f\"--- CHECK HALLUCINATION ---\")\n",
    "    documents = state[\"filtered_documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    is_grounded = hallucination_grader.grade(documents, generation)\n",
    "    print(f\"Grounded in facts: {is_grounded}\")\n",
    "    \n",
    "    return {}  # Just for routing, no state update\n",
    "\n",
    "def check_answer(state: SelfRAGState) -> dict:\n",
    "    \"\"\"Check if answer addresses the question.\"\"\"\n",
    "    print(f\"--- CHECK ANSWER ---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    is_useful = answer_grader.grade(question, generation)\n",
    "    print(f\"Addresses question: {is_useful}\")\n",
    "    \n",
    "    return {}  # Just for routing\n",
    "\n",
    "print(\"Check nodes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 5: Define Routing Functions\n",
    "\n",
    "Self-RAG uses conditional edges to route based on grading results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_grading(state: SelfRAGState) -> Literal[\"generate\", \"no_docs\"]:\n",
    "    \"\"\"Route based on whether we have relevant documents.\"\"\"\n",
    "    filtered = state.get(\"filtered_documents\", [])\n",
    "    if not filtered:\n",
    "        print(\"No relevant documents found\")\n",
    "        return \"no_docs\"\n",
    "    return \"generate\"\n",
    "\n",
    "def route_after_hallucination_check(state: SelfRAGState) -> Literal[\"check_answer\", \"retry\"]:\n",
    "    \"\"\"Route based on hallucination check.\"\"\"\n",
    "    documents = state[\"filtered_documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    max_retries = state.get(\"max_retries\", 3)\n",
    "    \n",
    "    is_grounded = hallucination_grader.grade(documents, generation)\n",
    "    \n",
    "    if is_grounded:\n",
    "        return \"check_answer\"\n",
    "    elif retry_count < max_retries:\n",
    "        print(f\"Hallucination detected, retrying ({retry_count + 1}/{max_retries})\")\n",
    "        return \"retry\"\n",
    "    else:\n",
    "        print(\"Max retries reached, proceeding anyway\")\n",
    "        return \"check_answer\"\n",
    "\n",
    "def route_after_answer_check(state: SelfRAGState) -> Literal[\"end\", \"retry\"]:\n",
    "    \"\"\"Route based on answer quality check.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    max_retries = state.get(\"max_retries\", 3)\n",
    "    \n",
    "    is_useful = answer_grader.grade(question, generation)\n",
    "    \n",
    "    if is_useful:\n",
    "        return \"end\"\n",
    "    elif retry_count < max_retries:\n",
    "        print(f\"Answer not useful, retrying ({retry_count + 1}/{max_retries})\")\n",
    "        return \"retry\"\n",
    "    else:\n",
    "        print(\"Max retries reached, returning current answer\")\n",
    "        return \"end\"\n",
    "\n",
    "print(\"Routing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_retry(state: SelfRAGState) -> dict:\n",
    "    \"\"\"Increment retry counter.\"\"\"\n",
    "    return {\"retry_count\": state.get(\"retry_count\", 0) + 1}\n",
    "\n",
    "def handle_no_docs(state: SelfRAGState) -> dict:\n",
    "    \"\"\"Handle case when no relevant documents found.\"\"\"\n",
    "    return {\n",
    "        \"generation\": \"I could not find any relevant documents to answer this question. Please try rephrasing or ask about a topic covered in the indexed documents.\"\n",
    "    }\n",
    "\n",
    "print(\"Helper nodes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Step 6: Build the Self-RAG Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(SelfRAGState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "graph_builder.add_node(\"generate\", generate)\n",
    "graph_builder.add_node(\"handle_no_docs\", handle_no_docs)\n",
    "graph_builder.add_node(\"increment_retry\", increment_retry)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# Conditional edge after grading\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    route_after_grading,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"no_docs\": \"handle_no_docs\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# No docs path ends\n",
    "graph_builder.add_edge(\"handle_no_docs\", END)\n",
    "\n",
    "# After generation, check hallucination\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    route_after_hallucination_check,\n",
    "    {\n",
    "        \"check_answer\": END,  # Simplified: go to end if grounded\n",
    "        \"retry\": \"increment_retry\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Retry loops back to generate\n",
    "graph_builder.add_edge(\"increment_retry\", \"generate\")\n",
    "\n",
    "# Compile\n",
    "self_rag_graph = graph_builder.compile()\n",
    "\n",
    "print(\"Self-RAG graph compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(self_rag_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not render graph: {e}\")\n",
    "    print(self_rag_graph.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Step 7: Test Self-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a relevant question\n",
    "question = \"What is Self-RAG and how does it improve upon traditional RAG?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = self_rag_graph.invoke({\n",
    "    \"question\": question,\n",
    "    \"retry_count\": 0,\n",
    "    \"max_retries\": 2,\n",
    "})\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nFINAL ANSWER:\")\n",
    "print(result[\"generation\"])\n",
    "print(f\"\\nUsed {len(result.get('filtered_documents', []))} relevant documents\")\n",
    "print(f\"Retries: {result.get('retry_count', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an irrelevant question (should handle gracefully)\n",
    "irrelevant_question = \"What is the best recipe for chocolate cake?\"\n",
    "\n",
    "print(f\"Question: {irrelevant_question}\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result2 = self_rag_graph.invoke({\n",
    "    \"question\": irrelevant_question,\n",
    "    \"retry_count\": 0,\n",
    "    \"max_retries\": 2,\n",
    "})\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nFINAL ANSWER:\")\n",
    "print(result2[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Complete Self-RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Self-RAG Implementation\n",
    "\n",
    "from typing import List, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "from langgraph_ollama_local.rag import (\n",
    "    LocalRetriever,\n",
    "    DocumentGrader,\n",
    "    HallucinationGrader,\n",
    "    AnswerGrader,\n",
    ")\n",
    "\n",
    "# 1. State\n",
    "class SelfRAGState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    filtered_documents: List[Document]\n",
    "    generation: str\n",
    "    retry_count: int\n",
    "    max_retries: int\n",
    "\n",
    "# 2. Components\n",
    "config = LocalAgentConfig()\n",
    "llm = ChatOllama(model=config.ollama.model, base_url=config.ollama.base_url, temperature=0)\n",
    "retriever = LocalRetriever()\n",
    "doc_grader = DocumentGrader(llm)\n",
    "hallucination_grader = HallucinationGrader(llm)\n",
    "\n",
    "# 3. Prompt\n",
    "PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer from context only. If unknown, say so.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 4. Nodes\n",
    "def retrieve(state): \n",
    "    return {\"documents\": retriever.retrieve_documents(state[\"question\"], k=5)}\n",
    "\n",
    "def grade_docs(state):\n",
    "    relevant, _ = doc_grader.grade_documents(state[\"documents\"], state[\"question\"])\n",
    "    return {\"filtered_documents\": relevant}\n",
    "\n",
    "def generate(state):\n",
    "    if not state[\"filtered_documents\"]:\n",
    "        return {\"generation\": \"No relevant documents found.\"}\n",
    "    context = \"\\n\".join([d.page_content for d in state[\"filtered_documents\"]])\n",
    "    response = llm.invoke(PROMPT.format_messages(context=context, question=state[\"question\"]))\n",
    "    return {\"generation\": response.content}\n",
    "\n",
    "def retry(state):\n",
    "    return {\"retry_count\": state.get(\"retry_count\", 0) + 1}\n",
    "\n",
    "# 5. Routing\n",
    "def check_and_route(state) -> str:\n",
    "    if not state[\"filtered_documents\"]:\n",
    "        return \"end\"\n",
    "    is_grounded = hallucination_grader.grade(state[\"filtered_documents\"], state[\"generation\"])\n",
    "    if is_grounded or state.get(\"retry_count\", 0) >= state.get(\"max_retries\", 2):\n",
    "        return \"end\"\n",
    "    return \"retry\"\n",
    "\n",
    "# 6. Build graph\n",
    "g = StateGraph(SelfRAGState)\n",
    "g.add_node(\"retrieve\", retrieve)\n",
    "g.add_node(\"grade\", grade_docs)\n",
    "g.add_node(\"generate\", generate)\n",
    "g.add_node(\"retry\", retry)\n",
    "g.add_edge(START, \"retrieve\")\n",
    "g.add_edge(\"retrieve\", \"grade\")\n",
    "g.add_edge(\"grade\", \"generate\")\n",
    "g.add_conditional_edges(\"generate\", check_and_route, {\"end\": END, \"retry\": \"retry\"})\n",
    "g.add_edge(\"retry\", \"generate\")\n",
    "\n",
    "self_rag = g.compile()\n",
    "\n",
    "# 7. Use it\n",
    "result = self_rag.invoke({\"question\": \"What is Self-RAG?\", \"retry_count\": 0, \"max_retries\": 2})\n",
    "print(result[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Key Concepts Recap\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **DocumentGrader** | Filters irrelevant retrieved documents |\n",
    "| **HallucinationGrader** | Checks if answer is grounded in facts |\n",
    "| **AnswerGrader** | Verifies answer addresses the question |\n",
    "| **Retry Logic** | Re-generates when quality checks fail |\n",
    "| **Conditional Edges** | Routes based on grading results |\n",
    "\n",
    "## Self-RAG vs Basic RAG\n",
    "\n",
    "| Aspect | Basic RAG | Self-RAG |\n",
    "|--------|-----------|----------|\n",
    "| Document filtering | None | Graded for relevance |\n",
    "| Hallucination check | None | LLM-based verification |\n",
    "| Answer quality | Assumed | Verified |\n",
    "| Error recovery | None | Retry mechanism |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In [Tutorial 10: CRAG](10_crag.ipynb), you'll learn:\n",
    "- Web search as a fallback when retrieval fails\n",
    "- Combining multiple knowledge sources\n",
    "- Corrective retrieval strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
