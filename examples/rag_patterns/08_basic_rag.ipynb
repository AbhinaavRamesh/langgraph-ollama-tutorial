{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 08: Basic RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "In this tutorial, you'll build a foundational RAG system that retrieves relevant documents and uses them to generate informed responses.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Document Loading**: Loading PDFs and text files\n",
    "- **Chunking**: Splitting documents into searchable pieces\n",
    "- **Embeddings**: Converting text to vectors with sentence-transformers\n",
    "- **Vector Storage**: Storing and querying with ChromaDB\n",
    "- **RAG Pipeline**: Combining retrieval with generation in LangGraph\n",
    "\n",
    "By the end, you'll have a working RAG system that answers questions using your local documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Why RAG?\n",
    "\n",
    "Large Language Models have knowledge cutoffs and can hallucinate. RAG solves this by:\n",
    "\n",
    "1. **Retrieving** relevant documents for the user's question\n",
    "2. **Augmenting** the prompt with this context\n",
    "3. **Generating** an answer grounded in the retrieved information\n",
    "\n",
    "```\n",
    "User Question → Retrieve Documents → Augment Prompt → Generate Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Make sure you have:\n",
    "1. Ollama running with a model pulled\n",
    "2. RAG dependencies installed: `pip install -e \".[rag]\"`\n",
    "3. PDF files in the `sources/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "import os\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "print(f\"Ollama server: {config.ollama.base_url}\")\n",
    "print(f\"Model: {config.ollama.model}\")\n",
    "\n",
    "# Check sources directory\n",
    "sources_dir = \"../../sources\"\n",
    "if os.path.exists(sources_dir):\n",
    "    files = os.listdir(sources_dir)\n",
    "    print(f\"\\nFound {len(files)} files in sources/\")\n",
    "    for f in files[:5]:\n",
    "        print(f\"  - {f}\")\n",
    "else:\n",
    "    print(\"\\nWarning: sources/ directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 1: Load Documents\n",
    "\n",
    "First, we need to load our source documents. The `DocumentLoader` class handles PDFs, text files, and markdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local.rag import DocumentLoader\n",
    "\n",
    "# Create loader\n",
    "loader = DocumentLoader()\n",
    "\n",
    "# Load all documents from sources/\n",
    "documents = loader.load_directory(\"../../sources\")\n",
    "\n",
    "print(f\"Loaded {len(documents)} document pages\")\n",
    "print(f\"\\nFirst document preview:\")\n",
    "print(f\"  Source: {documents[0].metadata.get('source', 'unknown')}\")\n",
    "print(f\"  Page: {documents[0].metadata.get('page', 'N/A')}\")\n",
    "print(f\"  Content: {documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Chunk Documents\n",
    "\n",
    "Documents need to be split into smaller chunks for effective retrieval. Key considerations:\n",
    "\n",
    "- **Chunk size**: ~1000 characters is a good starting point\n",
    "- **Overlap**: Some overlap (200 chars) ensures context isn't lost at boundaries\n",
    "- **Sentence boundaries**: Try to break at natural points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "from langgraph_ollama_local.rag import DocumentIndexer\nfrom langgraph_ollama_local.rag.indexer import IndexerConfig\n\n# Configure chunking (using default collection \"documents\")\nindexer_config = IndexerConfig(\n    chunk_size=1000,       # Characters per chunk\n    chunk_overlap=200,     # Overlap between chunks\n    embedding_model=\"all-mpnet-base-v2\",\n)\n\n# Create indexer\nindexer = DocumentIndexer(config=indexer_config)\n\n# Chunk the documents\nchunks = indexer.chunk_documents(documents)\n\nprint(f\"Created {len(chunks)} chunks from {len(documents)} pages\")\nprint(f\"\\nExample chunk:\")\nprint(f\"  Source: {chunks[0].metadata.get('filename', 'unknown')}\")\nprint(f\"  Chunk index: {chunks[0].metadata.get('chunk_index', 0)}\")\nprint(f\"  Length: {len(chunks[0].page_content)} characters\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. We use `sentence-transformers` for local embedding generation:\n",
    "\n",
    "| Model | Dimensions | Quality | Speed |\n",
    "|-------|------------|---------|-------|\n",
    "| `all-mpnet-base-v2` | 768 | High | Medium |\n",
    "| `all-MiniLM-L6-v2` | 384 | Good | Fast |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_ollama_local.rag import LocalEmbeddings\n",
    "\n",
    "# Create embedding model\n",
    "embeddings = LocalEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# Test embedding generation\n",
    "test_texts = [\n",
    "    \"What is Retrieval-Augmented Generation?\",\n",
    "    \"RAG combines retrieval with generation.\",\n",
    "]\n",
    "\n",
    "vectors = embeddings.embed_documents(test_texts)\n",
    "\n",
    "print(f\"Model: {embeddings.model_name}\")\n",
    "print(f\"Dimensions: {embeddings.dimensions}\")\n",
    "print(f\"\\nGenerated {len(vectors)} embeddings\")\n",
    "print(f\"First vector shape: {len(vectors[0])} dimensions\")\n",
    "print(f\"First 5 values: {vectors[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Store in ChromaDB\n",
    "\n",
    "ChromaDB is a local vector database that stores embeddings and enables similarity search. It persists to disk so you don't need to re-index every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index all chunks into ChromaDB\n",
    "# This generates embeddings and stores them\n",
    "num_indexed = indexer.index_documents(chunks)\n",
    "\n",
    "print(f\"Indexed {num_indexed} document chunks\")\n",
    "\n",
    "# Check stats\n",
    "stats = indexer.get_stats()\n",
    "print(f\"\\nCollection stats:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Query the Index\n",
    "\n",
    "Now we can search for relevant documents using semantic similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "from langgraph_ollama_local.rag import LocalRetriever\nfrom langgraph_ollama_local.rag.retriever import RetrieverConfig\n\n# Create retriever (using same default collection as indexer)\nretriever_config = RetrieverConfig(\n    default_k=4,  # Return top 4 results\n)\nretriever = LocalRetriever(config=retriever_config)\n\n# Search for relevant documents\nquery = \"What is Self-RAG and how does it work?\"\nresults = retriever.retrieve(query, k=3)\n\nprint(f\"Query: {query}\\n\")\nprint(f\"Found {len(results)} relevant documents:\\n\")\n\nfor i, (doc, score) in enumerate(results, 1):\n    print(f\"--- Result {i} (score: {score:.3f}) ---\")\n    print(f\"Source: {doc.metadata.get('filename', 'unknown')}\")\n    print(f\"Content: {doc.page_content[:300]}...\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 6: Build the RAG Graph\n",
    "\n",
    "Now let's build a LangGraph that combines retrieval and generation:\n",
    "\n",
    "```\n",
    "START → retrieve → generate → END\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Define the state for our RAG graph\n",
    "class RAGState(TypedDict):\n",
    "    \"\"\"State for the RAG pipeline.\"\"\"\n",
    "    question: str                    # User's question\n",
    "    documents: List[Document]        # Retrieved documents\n",
    "    generation: str                  # Generated answer\n",
    "\n",
    "print(\"State schema defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0,  # More deterministic for RAG\n",
    ")\n",
    "\n",
    "print(f\"Using model: {config.ollama.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the retrieve node\n",
    "def retrieve(state: RAGState) -> dict:\n",
    "    \"\"\"Retrieve relevant documents for the question.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Get documents (without scores for simplicity)\n",
    "    docs = retriever.retrieve_documents(question, k=4)\n",
    "    \n",
    "    print(f\"Retrieved {len(docs)} documents\")\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "print(\"Retrieve node defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG prompt template\n",
    "RAG_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following retrieved context to answer the question.\n",
    "If you don't know the answer from the context, say so.\n",
    "Keep your answer concise and focused.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "\n",
    "# Define the generate node\n",
    "def generate(state: RAGState) -> dict:\n",
    "    \"\"\"Generate an answer using retrieved documents.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Format documents into context\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}:\\n{doc.page_content}\"\n",
    "        for i, doc in enumerate(documents)\n",
    "    ])\n",
    "    \n",
    "    # Generate response\n",
    "    messages = rag_prompt.format_messages(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return {\"generation\": response.content}\n",
    "\n",
    "print(\"Generate node defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "graph_builder = StateGraph(RAGState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"generate\", generate)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "rag_graph = graph_builder.compile()\n",
    "\n",
    "print(\"RAG graph compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(rag_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not render graph: {e}\")\n",
    "    print(rag_graph.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Step 7: Test the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG pipeline\n",
    "question = \"What is Self-RAG and how does it improve upon traditional RAG?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Processing...\\n\")\n",
    "\n",
    "result = rag_graph.invoke({\"question\": question})\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 50)\n",
    "print(result[\"generation\"])\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Based on {len(result['documents'])} retrieved documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another question\n",
    "question2 = \"What are the key components of a CRAG system?\"\n",
    "\n",
    "print(f\"Question: {question2}\\n\")\n",
    "\n",
    "result2 = rag_graph.invoke({\"question\": question2})\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(result2[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Step 8: Add Source Citations\n",
    "\n",
    "A key feature of RAG is being able to cite sources. Let's enhance our pipeline to include citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sources(documents: List[Document]) -> str:\n",
    "    \"\"\"Format document sources for citation.\"\"\"\n",
    "    sources = []\n",
    "    seen = set()\n",
    "    \n",
    "    for doc in documents:\n",
    "        filename = doc.metadata.get('filename', 'Unknown')\n",
    "        page = doc.metadata.get('page', '')\n",
    "        \n",
    "        # Create unique source identifier\n",
    "        source_id = f\"{filename}:{page}\"\n",
    "        if source_id not in seen:\n",
    "            seen.add(source_id)\n",
    "            if page:\n",
    "                sources.append(f\"- {filename} (page {page})\")\n",
    "            else:\n",
    "                sources.append(f\"- {filename}\")\n",
    "    \n",
    "    return \"\\n\".join(sources)\n",
    "\n",
    "# Test with our last result\n",
    "print(\"Sources:\")\n",
    "print(format_sources(result2[\"documents\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, show_sources: bool = True) -> str:\n",
    "    \"\"\"Complete RAG query with optional source display.\"\"\"\n",
    "    result = rag_graph.invoke({\"question\": question})\n",
    "    \n",
    "    output = []\n",
    "    output.append(f\"Question: {question}\")\n",
    "    output.append(\"\")\n",
    "    output.append(\"Answer:\")\n",
    "    output.append(result[\"generation\"])\n",
    "    \n",
    "    if show_sources:\n",
    "        output.append(\"\")\n",
    "        output.append(\"Sources:\")\n",
    "        output.append(format_sources(result[\"documents\"]))\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# Test the helper function\n",
    "print(rag_query(\"What techniques does Adaptive RAG use?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Complete Code\n",
    "\n",
    "Here's the complete Basic RAG implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "# Complete Basic RAG Implementation\n\nfrom typing import List\nfrom typing_extensions import TypedDict\nfrom langchain_core.documents import Document\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_ollama import ChatOllama\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph_ollama_local import LocalAgentConfig\nfrom langgraph_ollama_local.rag import (\n    DocumentLoader,\n    DocumentIndexer,\n    LocalRetriever,\n)\n\n# 1. State definition\nclass RAGState(TypedDict):\n    question: str\n    documents: List[Document]\n    generation: str\n\n# 2. Setup components\nconfig = LocalAgentConfig()\nllm = ChatOllama(\n    model=config.ollama.model,\n    base_url=config.ollama.base_url,\n    temperature=0,\n)\n\n# Retriever uses default \"documents\" collection\nretriever = LocalRetriever()\n\n# 3. RAG prompt\nRAG_PROMPT = ChatPromptTemplate.from_template(\n    \"\"\"Answer based on the context. If unknown, say so.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n)\n\n# 4. Node functions\ndef retrieve(state: RAGState) -> dict:\n    docs = retriever.retrieve_documents(state[\"question\"], k=4)\n    return {\"documents\": docs}\n\ndef generate(state: RAGState) -> dict:\n    context = \"\\n\\n\".join([d.page_content for d in state[\"documents\"]])\n    messages = RAG_PROMPT.format_messages(\n        context=context, question=state[\"question\"]\n    )\n    response = llm.invoke(messages)\n    return {\"generation\": response.content}\n\n# 5. Build graph\ngraph = StateGraph(RAGState)\ngraph.add_node(\"retrieve\", retrieve)\ngraph.add_node(\"generate\", generate)\ngraph.add_edge(START, \"retrieve\")\ngraph.add_edge(\"retrieve\", \"generate\")\ngraph.add_edge(\"generate\", END)\nrag_app = graph.compile()\n\n# 6. Use it!\nresult = rag_app.invoke({\"question\": \"What is RAG?\"})\nprint(result[\"generation\"])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Key Concepts Recap\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **DocumentLoader** | Loads PDFs, text, markdown files |\n",
    "| **Chunking** | Splits documents into searchable pieces |\n",
    "| **Embeddings** | Converts text to vectors (semantic meaning) |\n",
    "| **ChromaDB** | Stores vectors and enables similarity search |\n",
    "| **Retriever** | Finds relevant documents for a query |\n",
    "| **RAG Prompt** | Combines context with question for LLM |\n",
    "\n",
    "## Limitations of Basic RAG\n",
    "\n",
    "Basic RAG has some limitations:\n",
    "\n",
    "1. **No quality check**: Retrieved documents might not be relevant\n",
    "2. **No hallucination detection**: LLM might still hallucinate\n",
    "3. **Single retrieval**: One-shot retrieval might miss information\n",
    "4. **No fallback**: No alternative if retrieval fails\n",
    "\n",
    "We'll address these in the upcoming tutorials:\n",
    "- **Self-RAG** (Tutorial 09): Grades documents and answers\n",
    "- **CRAG** (Tutorial 10): Web search fallback\n",
    "- **Adaptive RAG** (Tutorial 11): Routes to best strategy\n",
    "- **Agentic RAG** (Tutorial 12): Multi-step retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Tune chunk size**: Try different chunk sizes (500, 1500) and see how it affects retrieval quality\n",
    "2. **Adjust k**: Retrieve more/fewer documents and observe the impact\n",
    "3. **Different embedding model**: Try `all-MiniLM-L6-v2` for faster embeddings\n",
    "4. **Add metadata filtering**: Filter by source file or page number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "In [Tutorial 09: Self-RAG](09_self_rag.ipynb), you'll learn:\n",
    "- How to grade retrieved documents for relevance\n",
    "- Detecting hallucinations in generated answers\n",
    "- Building retry loops for improved quality\n",
    "- The Self-RAG architecture pattern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}