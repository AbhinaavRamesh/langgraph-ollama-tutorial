{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02: Tool Calling & ReAct Agent\n",
    "\n",
    "In this tutorial, you'll build a ReAct (Reasoning + Acting) agent from scratch. This pattern is the foundation of most LLM agents.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to define **tools** that your agent can use\n",
    "- **Binding tools** to your LLM\n",
    "- **Conditional edges** for routing based on LLM decisions\n",
    "- The **ReAct loop**: Agent → Tools → Agent → ...\n",
    "- Building an agent that can take real actions\n",
    "\n",
    "By the end, you'll have an agent that can reason about problems and use tools to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The ReAct Pattern\n\nReAct stands for **Re**asoning and **Act**ing. This pattern, introduced in the paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629), is the foundation of most LLM agents.\n\n### Why ReAct?\n\nTraditional LLMs can only generate text. But what if we want them to:\n- Search the web for current information?\n- Perform calculations?\n- Access databases?\n- Take real-world actions?\n\nReAct solves this by giving LLMs access to **tools** and letting them **decide** when to use them.\n\n### The Loop\n\n1. **User Query** → Agent receives a question or task\n2. **Reasoning** → Agent thinks about what to do (may generate internal thoughts)\n3. **Action** → Agent decides to call a tool OR respond to user\n4. **Observation** → Tool returns results\n5. **Repeat** → Back to reasoning with new information\n\nThis continues until the agent has enough information to answer.\n\n### Visual Overview\n\nThe graph visualization below (generated in Step 8) shows this loop:\n- **agent** node: Calls the LLM to reason and decide\n- **tools** node: Executes requested tools\n- Conditional edge: Routes based on whether tools were called"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Verify Ollama connection\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "print(f\"Ollama: {config.ollama.base_url}\")\n",
    "print(f\"Model: {config.ollama.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Tools\n",
    "\n",
    "Tools are functions that your agent can call. Each tool needs:\n",
    "- A clear **name**\n",
    "- A **docstring** describing what it does (the LLM reads this!)\n",
    "- **Type hints** for arguments\n",
    "\n",
    "Let's create some simple tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers together.\n",
    "    \n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "        \n",
    "    Returns:\n",
    "        The sum of a and b\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\n",
    "    \n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "        \n",
    "    Returns:\n",
    "        The product of a and b\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the current weather for a location.\n",
    "    \n",
    "    Args:\n",
    "        location: City name or location\n",
    "        \n",
    "    Returns:\n",
    "        Weather description for the location\n",
    "    \"\"\"\n",
    "    # Mock implementation - in real use, call a weather API\n",
    "    weather_data = {\n",
    "        \"san francisco\": \"Sunny, 68°F\",\n",
    "        \"new york\": \"Cloudy, 55°F\",\n",
    "        \"london\": \"Rainy, 50°F\",\n",
    "    }\n",
    "    location_lower = location.lower()\n",
    "    for city, weather in weather_data.items():\n",
    "        if city in location_lower:\n",
    "            return f\"Weather in {location}: {weather}\"\n",
    "    return f\"Weather data not available for {location}\"\n",
    "\n",
    "# Collect our tools\n",
    "tools = [add, multiply, get_weather]\n",
    "\n",
    "# Let's see what the tools look like\n",
    "for t in tools:\n",
    "    print(f\"Tool: {t.name}\")\n",
    "    print(f\"  Description: {t.description}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create LLM with Tool Binding\n",
    "\n",
    "We need to tell the LLM about our tools. This is done with `bind_tools()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Create base LLM\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0,  # Deterministic for tool calling\n",
    ")\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"LLM configured with tools!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tool Calling\n",
    "\n",
    "Let's see how the LLM decides to call tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Does the LLM want to call a tool?\n",
    "response = llm_with_tools.invoke(\"What is 25 times 4?\")\n",
    "\n",
    "print(f\"Response type: {type(response).__name__}\")\n",
    "print(f\"Content: {response.content}\")\n",
    "print(f\"Tool calls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define State\n",
    "\n",
    "Our agent state is the same as the chatbot - we track messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for our ReAct agent.\"\"\"\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Agent Node\n",
    "\n",
    "The agent node calls the LLM and returns its response. The LLM might respond with text OR request a tool call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# System prompt to guide the agent\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant with access to tools.\n",
    "\n",
    "When you need to perform calculations or get information, use the available tools.\n",
    "Always explain your reasoning before and after using tools.\n",
    "\n",
    "Available tools:\n",
    "- add: Add two numbers\n",
    "- multiply: Multiply two numbers  \n",
    "- get_weather: Get weather for a location\n",
    "\"\"\"\n",
    "\n",
    "def agent_node(state: AgentState) -> dict:\n",
    "    \"\"\"Call the LLM to decide what to do next.\n",
    "    \n",
    "    The LLM will either:\n",
    "    1. Return a text response (done)\n",
    "    2. Request one or more tool calls (continue)\n",
    "    \"\"\"\n",
    "    # Prepend system message\n",
    "    messages = [SystemMessage(content=SYSTEM_PROMPT)] + state[\"messages\"]\n",
    "    \n",
    "    # Call LLM with tools\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define the Tool Node\n",
    "\n",
    "The tool node executes tool calls requested by the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "# Create a lookup dictionary for tools\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "def tool_node(state: AgentState) -> dict:\n",
    "    \"\"\"Execute tool calls from the last message.\n",
    "    \n",
    "    This node:\n",
    "    1. Gets tool calls from the last AI message\n",
    "    2. Executes each tool\n",
    "    3. Returns ToolMessages with results\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    \n",
    "    # Get the last message (should be an AI message with tool calls)\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # Execute each tool call\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_id = tool_call[\"id\"]\n",
    "        \n",
    "        print(f\"  Executing: {tool_name}({tool_args})\")\n",
    "        \n",
    "        # Get the tool and execute it\n",
    "        tool = tools_by_name[tool_name]\n",
    "        result = tool.invoke(tool_args)\n",
    "        \n",
    "        print(f\"  Result: {result}\")\n",
    "        \n",
    "        # Create a ToolMessage with the result\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(result),\n",
    "                name=tool_name,\n",
    "                tool_call_id=tool_id,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return {\"messages\": outputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define Conditional Routing\n",
    "\n",
    "The key to ReAct is deciding **when to stop**. After the agent runs:\n",
    "- If it requested tool calls → route to tools\n",
    "- If it responded with text → we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Decide whether to continue to tools or end.\n",
    "    \n",
    "    Returns:\n",
    "        \"tools\" if agent requested tool calls\n",
    "        \"end\" if agent is done (no tool calls)\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # Check if the LLM made tool calls\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # No tool calls means we're done\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build the Graph\n",
    "\n",
    "Now we assemble everything into a graph with conditional edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add edges\n",
    "# START -> agent: Begin with the agent\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# agent -> conditional: Either go to tools or end\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",           # From node\n",
    "    should_continue,   # Condition function\n",
    "    {\n",
    "        \"tools\": \"tools\",  # If \"tools\", go to tools node\n",
    "        \"end\": END,        # If \"end\", finish\n",
    "    }\n",
    ")\n",
    "\n",
    "# tools -> agent: After tools, always go back to agent\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"ReAct agent compiled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not render graph: {e}\")\n",
    "    print(graph.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run the Agent!\n",
    "\n",
    "Let's test our ReAct agent with different queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(query: str):\n",
    "    \"\"\"Run the agent and print the conversation.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"User: {query}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Stream the response\n",
    "    for event in graph.stream(\n",
    "        {\"messages\": [(\"user\", query)]},\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        last_message = event[\"messages\"][-1]\n",
    "        \n",
    "        # Print based on message type\n",
    "        if hasattr(last_message, 'type'):\n",
    "            if last_message.type == \"ai\":\n",
    "                if last_message.tool_calls:\n",
    "                    print(f\"\\nAgent deciding to use tools...\")\n",
    "                    for tc in last_message.tool_calls:\n",
    "                        print(f\"  → {tc['name']}({tc['args']})\")\n",
    "                elif last_message.content:\n",
    "                    print(f\"\\nAgent: {last_message.content}\")\n",
    "            elif last_message.type == \"tool\":\n",
    "                print(f\"  Tool result: {last_message.content}\")\n",
    "\n",
    "# Test 1: Simple math\n",
    "run_agent(\"What is 25 times 4?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Multi-step calculation\n",
    "run_agent(\"What is 15 plus 27, then multiply the result by 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Weather query\n",
    "run_agent(\"What's the weather like in San Francisco?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: No tool needed\n",
    "run_agent(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Code\n",
    "\n",
    "Here's the complete ReAct agent in one cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete ReAct Agent Implementation\n",
    "\n",
    "import json\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, ToolMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "# === Tools ===\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [add, multiply]\n",
    "tools_by_name = {t.name: t for t in tools}\n",
    "\n",
    "# === LLM ===\n",
    "config = LocalAgentConfig()\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0,\n",
    ").bind_tools(tools)\n",
    "\n",
    "# === State ===\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# === Nodes ===\n",
    "def agent_node(state: AgentState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def tool_node(state: AgentState):\n",
    "    outputs = []\n",
    "    for tc in state[\"messages\"][-1].tool_calls:\n",
    "        result = tools_by_name[tc[\"name\"]].invoke(tc[\"args\"])\n",
    "        outputs.append(ToolMessage(\n",
    "            content=json.dumps(result),\n",
    "            name=tc[\"name\"],\n",
    "            tool_call_id=tc[\"id\"],\n",
    "        ))\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "# === Routing ===\n",
    "def should_continue(state: AgentState):\n",
    "    last = state[\"messages\"][-1]\n",
    "    if hasattr(last, \"tool_calls\") and last.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"end\"\n",
    "\n",
    "# === Graph ===\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", \"end\": END})\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "graph = workflow.compile()\n",
    "\n",
    "# === Test ===\n",
    "result = graph.invoke({\"messages\": [(\"user\", \"What is 7 times 8?\")]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Concepts Recap\n\n| Concept | Description |\n|---------|-------------|\n| **Tool** | A function decorated with `@tool` that the LLM can call |\n| **bind_tools()** | Tells the LLM what tools are available |\n| **tool_calls** | List of tool invocations requested by the LLM |\n| **ToolMessage** | Message containing a tool's output |\n| **Conditional Edge** | Edge that routes based on a condition function |\n| **ReAct Loop** | Agent → Tools → Agent cycle until done |\n\n## Common Issues with Local LLMs\n\n### Tool Calling Not Working?\n\nNot all local models support tool calling. Check the [Ollama Tools Models](https://ollama.com/search?c=tools) page for the official list.\n\n**Models that work well:**\n- `llama3.1:8b` or larger - Best overall for function calling\n- `llama3.2:3b` - Good for resource-constrained environments\n- `mistral:7b` - Efficient and reliable\n- `qwen3` - Featured in official Ollama docs\n- `granite4` - Tool-optimized by IBM\n\n### Inconsistent Results?\n\n- Set `temperature=0` for deterministic tool calling\n- Use explicit, detailed tool descriptions\n- Add examples to your system prompt\n\n## What's Next?\n\nIn [Tutorial 03: Memory & Persistence](03_memory_persistence.ipynb), you'll learn:\n- How to persist conversations across sessions\n- Using checkpointers (MemorySaver, SqliteSaver)\n- Thread IDs for multi-user support\n- Inspecting conversation history"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}