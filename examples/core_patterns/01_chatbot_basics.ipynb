{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: Build a Basic Chatbot\n",
    "\n",
    "In this tutorial, you'll build your first LangGraph chatbot running entirely on local hardware with Ollama.\n",
    "\n",
    "**What you'll learn:**\n",
    "- `StateGraph`: The core LangGraph abstraction\n",
    "- **Nodes**: Functions that process state\n",
    "- **Edges**: Connections between nodes\n",
    "- **State schema**: Defining what data flows through your graph\n",
    "- **Reducers**: How state updates are handled (e.g., `add_messages`)\n",
    "\n",
    "By the end, you'll understand the fundamental building blocks of LangGraph and have a working chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Make sure you have:\n",
    "1. Ollama running (locally or on your LAN)\n",
    "2. A model pulled (e.g., `ollama pull llama3.2:3b`)\n",
    "3. The tutorial package installed (`pip install -e .`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Ollama connection\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "print(f\"Ollama server: {config.ollama.base_url}\")\n",
    "print(f\"Model: {config.ollama.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the State\n",
    "\n",
    "Every LangGraph application starts with defining **State**. State is:\n",
    "- A `TypedDict` that defines the schema of data flowing through your graph\n",
    "- Shared between all nodes\n",
    "- Updated by nodes as they process\n",
    "\n",
    "For a chatbot, our state needs to track the conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The state of our chatbot.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: The conversation history. Uses `add_messages` reducer\n",
    "                  which appends new messages instead of overwriting.\n",
    "    \"\"\"\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is `add_messages`?\n",
    "\n",
    "The `Annotated[list, add_messages]` syntax tells LangGraph how to update this field:\n",
    "\n",
    "- **Without reducer**: New value overwrites old value\n",
    "- **With `add_messages`**: New messages are appended to existing list\n",
    "\n",
    "This is crucial for chatbots - we want to accumulate the conversation, not replace it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the LLM\n",
    "\n",
    "We'll use `ChatOllama` from LangChain to connect to our local Ollama server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Create LLM using our config\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=config.ollama.temperature,\n",
    ")\n",
    "\n",
    "# Quick test\n",
    "response = llm.invoke(\"Say 'hello' and nothing else.\")\n",
    "print(f\"LLM test: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Chatbot Node\n",
    "\n",
    "A **node** is a function that:\n",
    "1. Receives the current state as input\n",
    "2. Does some processing\n",
    "3. Returns updates to the state\n",
    "\n",
    "Our chatbot node will:\n",
    "1. Take the messages from state\n",
    "2. Send them to the LLM\n",
    "3. Return the LLM's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State) -> dict:\n",
    "    \"\"\"Process messages and generate a response.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state containing conversation messages\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with new messages to add to state\n",
    "    \"\"\"\n",
    "    # Invoke LLM with conversation history\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    \n",
    "    # Return the response - add_messages will append it\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the Graph\n",
    "\n",
    "Now we assemble the pieces into a `StateGraph`:\n",
    "\n",
    "1. Create a `StateGraph` with our State schema\n",
    "2. Add the chatbot node\n",
    "3. Connect edges (START → chatbot → END)\n",
    "4. Compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# 1. Create the graph with our State schema\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 2. Add the chatbot node\n",
    "#    First arg: unique node name\n",
    "#    Second arg: the function to call\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# 3. Add edges\n",
    "#    START -> chatbot: Begin execution at the chatbot node\n",
    "#    chatbot -> END: Finish after chatbot runs\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# 4. Compile the graph\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "print(\"Graph compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Graph\n",
    "\n",
    "LangGraph can render your graph as a diagram. This is incredibly helpful for understanding complex flows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not render graph: {e}\")\n",
    "    print(\"Install graphviz for visualization: brew install graphviz\")\n",
    "    # Fallback to ASCII\n",
    "    print(graph.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run the Chatbot\n",
    "\n",
    "Now let's use our chatbot! We invoke the graph with an initial message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single invocation\n",
    "result = graph.invoke({\n",
    "    \"messages\": [(\"user\", \"What is LangGraph?\")]\n",
    "})\n",
    "\n",
    "# Print the response\n",
    "print(\"User: What is LangGraph?\")\n",
    "print(f\"Assistant: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Streaming Responses\n",
    "\n",
    "For a better user experience, especially with slower local models, we can stream responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(user_input: str):\n",
    "    \"\"\"Stream the graph response for better UX.\"\"\"\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    \n",
    "    for event in graph.stream(\n",
    "        {\"messages\": [(\"user\", user_input)]},\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        # Get the last message\n",
    "        last_message = event[\"messages\"][-1]\n",
    "        # Only print AI messages (skip user message echo)\n",
    "        if hasattr(last_message, 'content') and last_message.type == \"ai\":\n",
    "            print(last_message.content)\n",
    "\n",
    "# Test streaming\n",
    "stream_response(\"Explain neural networks in 2 sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Interactive Chat Loop\n",
    "\n",
    "Let's create an interactive chat session. Note: this won't maintain memory between messages yet (we'll add that in Tutorial 03)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    \"\"\"Simple interactive chat loop.\"\"\"\n",
    "    print(\"Chat with your local LLM! Type 'quit' to exit.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Invoke graph\n",
    "            result = graph.invoke({\"messages\": [(\"user\", user_input)]})\n",
    "            print(f\"Assistant: {result['messages'][-1].content}\")\n",
    "            print()\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "\n",
    "# Uncomment to run interactive chat:\n",
    "# chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Code\n",
    "\n",
    "Here's the complete chatbot in one cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete basic chatbot implementation\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "# 1. State definition\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 2. LLM setup\n",
    "config = LocalAgentConfig()\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    ")\n",
    "\n",
    "# 3. Node definition\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# 4. Graph construction\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 5. Use it!\n",
    "result = graph.invoke({\"messages\": [(\"user\", \"Hello!\")]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts Recap\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **StateGraph** | The container for your graph definition |\n",
    "| **State** | TypedDict defining data that flows through the graph |\n",
    "| **Node** | A function that processes state and returns updates |\n",
    "| **Edge** | Connections between nodes (including START and END) |\n",
    "| **Reducer** | Function that determines how state updates are merged (e.g., `add_messages`) |\n",
    "| **compile()** | Converts the graph definition into an executable graph |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In [Tutorial 02: Tool Calling](02_tool_calling.ipynb), you'll learn:\n",
    "- How to define tools for your agent\n",
    "- The ReAct (Reasoning + Acting) pattern\n",
    "- Conditional edges for routing\n",
    "- Building agents that can take actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
