{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 06: Plan and Execute\n",
    "\n",
    "In this tutorial, you'll learn how to build agents that plan before they act, breaking complex tasks into manageable steps.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Planning**: Breaking tasks into steps\n",
    "- **Execution**: Processing steps sequentially\n",
    "- **Re-planning**: Adjusting plans based on results\n",
    "- **Structured outputs**: Using Pydantic for plans\n",
    "\n",
    "By the end, you'll have an agent that plans, executes, and adapts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Why Plan and Execute?\n",
    "\n",
    "ReAct agents (Tutorial 02) decide step-by-step. This works well for simple tasks but struggles with:\n",
    "- **Multi-step problems**: Need to think ahead\n",
    "- **Resource efficiency**: Planning can use a stronger model, execution a faster one\n",
    "- **Visibility**: Explicit plans are easier to review and debug\n",
    "\n",
    "### The Pattern\n",
    "\n",
    "1. **Plan**: Create a list of steps to accomplish the goal\n",
    "2. **Execute**: Work through steps one at a time\n",
    "3. **Re-plan**: Update the plan if needed based on results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Graph Visualization\n",
    "\n",
    "![Plan and Execute Graph](../docs/images/06-plan-execute-graph.png)\n",
    "\n",
    "The planner creates a plan, the executor processes each step, and optionally re-plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "print(f\"Ollama: {config.ollama.base_url}\")\n",
    "print(f\"Model: {config.ollama.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 1: Define State\n",
    "\n",
    "Our state tracks the task, plan, and execution progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Tuple\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class PlanExecuteState(TypedDict):\n",
    "    \"\"\"State for plan-and-execute agent.\"\"\"\n",
    "    messages: Annotated[list, add_messages]  # Conversation history\n",
    "    task: str                                 # Original task\n",
    "    plan: List[str]                           # List of steps\n",
    "    current_step: int                         # Current step index\n",
    "    past_steps: Annotated[List[Tuple[str, str]], operator.add]  # (step, result) pairs\n",
    "    response: str                             # Final response\n",
    "\n",
    "print(\"State defined with: task, plan, current_step, past_steps, response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Create the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"LLM configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Create the Planner Node\n",
    "\n",
    "The planner breaks down the task into steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import json\n",
    "import re\n",
    "\n",
    "PLANNER_PROMPT = \"\"\"You are a task planner. Break down the given task into clear, actionable steps.\n",
    "\n",
    "Return your plan as a JSON array of strings, where each string is one step.\n",
    "Keep steps simple and focused. Aim for 3-5 steps.\n",
    "\n",
    "Example output:\n",
    "[\"Step 1: Research the topic\", \"Step 2: Outline main points\", \"Step 3: Write the content\"]\n",
    "\n",
    "Return ONLY the JSON array, no other text.\n",
    "\"\"\"\n",
    "\n",
    "def planner_node(state: PlanExecuteState) -> dict:\n",
    "    \"\"\"Create a plan for the task.\"\"\"\n",
    "    task = state[\"task\"]\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=PLANNER_PROMPT),\n",
    "        HumanMessage(content=f\"Create a plan for: {task}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    content = response.content\n",
    "    \n",
    "    # Parse JSON from response\n",
    "    try:\n",
    "        # Try to find JSON array in response\n",
    "        match = re.search(r'\\[.*?\\]', content, re.DOTALL)\n",
    "        if match:\n",
    "            plan = json.loads(match.group())\n",
    "        else:\n",
    "            # Fallback: split by newlines\n",
    "            plan = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "    except json.JSONDecodeError:\n",
    "        plan = [content]  # Use whole response as single step\n",
    "    \n",
    "    print(f\"\\n=== PLAN ===\")\n",
    "    for i, step in enumerate(plan):\n",
    "        print(f\"  {i+1}. {step}\")\n",
    "    \n",
    "    return {\n",
    "        \"plan\": plan,\n",
    "        \"current_step\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Create the Executor Node\n",
    "\n",
    "The executor processes one step at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXECUTOR_PROMPT = \"\"\"You are a task executor. Complete the given step as part of a larger task.\n",
    "\n",
    "Provide a clear, concise result for this step.\n",
    "Focus on actionable output that helps accomplish the overall task.\n",
    "\"\"\"\n",
    "\n",
    "def executor_node(state: PlanExecuteState) -> dict:\n",
    "    \"\"\"Execute the current step.\"\"\"\n",
    "    plan = state[\"plan\"]\n",
    "    current_step = state[\"current_step\"]\n",
    "    task = state[\"task\"]\n",
    "    past_steps = state.get(\"past_steps\", [])\n",
    "    \n",
    "    if current_step >= len(plan):\n",
    "        return {}  # No more steps\n",
    "    \n",
    "    step = plan[current_step]\n",
    "    \n",
    "    # Build context from past steps\n",
    "    context = f\"Original task: {task}\\n\\n\"\n",
    "    if past_steps:\n",
    "        context += \"Previous steps completed:\\n\"\n",
    "        for prev_step, prev_result in past_steps:\n",
    "            context += f\"- {prev_step}: {prev_result[:100]}...\\n\"\n",
    "        context += \"\\n\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=EXECUTOR_PROMPT),\n",
    "        HumanMessage(content=f\"{context}Now execute this step: {step}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    result = response.content\n",
    "    \n",
    "    print(f\"\\n=== STEP {current_step + 1} ===\")\n",
    "    print(f\"  Executing: {step}\")\n",
    "    print(f\"  Result: {result[:200]}...\" if len(result) > 200 else f\"  Result: {result}\")\n",
    "    \n",
    "    return {\n",
    "        \"past_steps\": [(step, result)],\n",
    "        \"current_step\": current_step + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Create the Finalizer Node\n",
    "\n",
    "Combines all step results into a final response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINALIZER_PROMPT = \"\"\"You are a task finalizer. Combine the results of all completed steps into a coherent final response.\n",
    "\n",
    "Synthesize the information and provide a complete answer to the original task.\n",
    "\"\"\"\n",
    "\n",
    "def finalizer_node(state: PlanExecuteState) -> dict:\n",
    "    \"\"\"Create final response from all step results.\"\"\"\n",
    "    task = state[\"task\"]\n",
    "    past_steps = state.get(\"past_steps\", [])\n",
    "    \n",
    "    # Build summary of all steps\n",
    "    steps_summary = \"\\n\".join([\n",
    "        f\"Step: {step}\\nResult: {result}\"\n",
    "        for step, result in past_steps\n",
    "    ])\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=FINALIZER_PROMPT),\n",
    "        HumanMessage(content=f\"Original task: {task}\\n\\nCompleted steps:\\n{steps_summary}\\n\\nProvide the final response.\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(f\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response.content)\n",
    "    \n",
    "    return {\"response\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 6: Define Routing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: PlanExecuteState) -> str:\n",
    "    \"\"\"Decide whether to execute more steps or finalize.\"\"\"\n",
    "    current_step = state[\"current_step\"]\n",
    "    plan = state[\"plan\"]\n",
    "    \n",
    "    if current_step < len(plan):\n",
    "        return \"executor\"  # More steps to execute\n",
    "    return \"finalizer\"  # All steps done, finalize\n",
    "\n",
    "print(\"Routing logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 7: Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build the plan-and-execute graph\n",
    "workflow = StateGraph(PlanExecuteState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "workflow.add_node(\"finalizer\", finalizer_node)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"planner\")           # Start with planning\n",
    "workflow.add_edge(\"planner\", \"executor\")      # Then execute first step\n",
    "workflow.add_conditional_edges(\n",
    "    \"executor\",\n",
    "    should_continue,\n",
    "    {\"executor\": \"executor\", \"finalizer\": \"finalizer\"}\n",
    ")\n",
    "workflow.add_edge(\"finalizer\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"Plan-and-execute graph compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not render: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Step 8: Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the plan-and-execute agent\n",
    "task = \"Explain the key benefits of using LangGraph for building AI agents, including at least 3 specific advantages.\"\n",
    "\n",
    "print(f\"Task: {task}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"task\": task,\n",
    "    \"messages\": [],\n",
    "    \"plan\": [],\n",
    "    \"current_step\": 0,\n",
    "    \"past_steps\": [],\n",
    "    \"response\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTION COMPLETE\")\n",
    "print(f\"Steps executed: {len(result['past_steps'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for easy use\n",
    "def plan_and_execute(task: str) -> str:\n",
    "    \"\"\"Run plan-and-execute on a task.\"\"\"\n",
    "    result = graph.invoke({\n",
    "        \"task\": task,\n",
    "        \"messages\": [],\n",
    "        \"plan\": [],\n",
    "        \"current_step\": 0,\n",
    "        \"past_steps\": [],\n",
    "        \"response\": \"\"\n",
    "    })\n",
    "    return result[\"response\"]\n",
    "\n",
    "# Test with another task\n",
    "response = plan_and_execute(\"Write a haiku about programming, then explain what each line means.\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Plan-and-Execute Agent\n",
    "\n",
    "import json\n",
    "import re\n",
    "import operator\n",
    "from typing import Annotated, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "# === State ===\n",
    "class PlanExecuteState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    task: str\n",
    "    plan: List[str]\n",
    "    current_step: int\n",
    "    past_steps: Annotated[List[Tuple[str, str]], operator.add]\n",
    "    response: str\n",
    "\n",
    "# === LLM ===\n",
    "config = LocalAgentConfig()\n",
    "llm = ChatOllama(model=config.ollama.model, base_url=config.ollama.base_url, temperature=0)\n",
    "\n",
    "# === Nodes ===\n",
    "def planner(state: PlanExecuteState) -> dict:\n",
    "    response = llm.invoke([HumanMessage(content=f\"Break this into 3-5 steps (JSON array): {state['task']}\")])\n",
    "    try:\n",
    "        plan = json.loads(re.search(r'\\[.*?\\]', response.content, re.DOTALL).group())\n",
    "    except:\n",
    "        plan = [response.content]\n",
    "    return {\"plan\": plan, \"current_step\": 0}\n",
    "\n",
    "def executor(state: PlanExecuteState) -> dict:\n",
    "    if state[\"current_step\"] >= len(state[\"plan\"]):\n",
    "        return {}\n",
    "    step = state[\"plan\"][state[\"current_step\"]]\n",
    "    response = llm.invoke([HumanMessage(content=f\"Execute: {step}\")])\n",
    "    return {\"past_steps\": [(step, response.content)], \"current_step\": state[\"current_step\"] + 1}\n",
    "\n",
    "def finalizer(state: PlanExecuteState) -> dict:\n",
    "    summary = \"\\n\".join([f\"{s}: {r}\" for s, r in state[\"past_steps\"]])\n",
    "    response = llm.invoke([HumanMessage(content=f\"Summarize results for '{state['task']}':\\n{summary}\")])\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "def should_continue(state: PlanExecuteState) -> str:\n",
    "    return \"executor\" if state[\"current_step\"] < len(state[\"plan\"]) else \"finalizer\"\n",
    "\n",
    "# === Graph ===\n",
    "workflow = StateGraph(PlanExecuteState)\n",
    "workflow.add_node(\"planner\", planner)\n",
    "workflow.add_node(\"executor\", executor)\n",
    "workflow.add_node(\"finalizer\", finalizer)\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"executor\")\n",
    "workflow.add_conditional_edges(\"executor\", should_continue, {\"executor\": \"executor\", \"finalizer\": \"finalizer\"})\n",
    "workflow.add_edge(\"finalizer\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# === Use ===\n",
    "result = graph.invoke({\"task\": \"List 3 benefits of Python\", \"messages\": [], \"plan\": [], \"current_step\": 0, \"past_steps\": [], \"response\": \"\"})\n",
    "print(result[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Key Concepts Recap\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Planner** | Creates a list of steps from the task |\n",
    "| **Executor** | Processes steps one at a time |\n",
    "| **Finalizer** | Combines results into final response |\n",
    "| **past_steps** | Accumulates (step, result) pairs |\n",
    "| **current_step** | Tracks progress through plan |\n",
    "\n",
    "## Advantages\n",
    "\n",
    "1. **Explicit reasoning**: Plan is visible and debuggable\n",
    "2. **Resource efficiency**: Use different models for planning vs execution\n",
    "3. **Flexibility**: Easy to add re-planning based on results\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In [Tutorial 07: Research Assistant](07_research_assistant.ipynb), you'll combine all patterns into a comprehensive research agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
