{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 05: Reflection\n",
    "\n",
    "In this tutorial, you'll learn how to build self-critiquing agents that iteratively improve their outputs through reflection.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Reflection loops**: Generate → Critique → Revise\n",
    "- **Self-improvement**: Using LLM to evaluate its own outputs\n",
    "- **Iteration control**: When to stop refining\n",
    "- **Quality enhancement**: Producing better outputs through feedback\n",
    "\n",
    "By the end, you'll have an agent that writes, critiques, and revises its own work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is Reflection?\n",
    "\n",
    "Reflection is a pattern where an LLM:\n",
    "1. **Generates** an initial output\n",
    "2. **Critiques** its own work (or has another LLM critique it)\n",
    "3. **Revises** based on the critique\n",
    "4. **Repeats** until satisfied\n",
    "\n",
    "This mirrors how humans improve their work through drafts and revisions.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Writing**: Essays, reports, documentation\n",
    "- **Code**: Generate, review, refactor\n",
    "- **Analysis**: Initial assessment, deeper analysis, conclusions\n",
    "- **Problem-solving**: Solution, evaluation, refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Graph Visualization\n",
    "\n",
    "![Reflection Graph](../docs/images/05-reflection-graph.png)\n",
    "\n",
    "The graph cycles between generation and critique until the output is satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "print(f\"Ollama: {config.ollama.base_url}\")\n",
    "print(f\"Model: {config.ollama.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 1: Define State\n",
    "\n",
    "Our reflection state tracks:\n",
    "- Messages (conversation history)\n",
    "- Current draft\n",
    "- Latest critique\n",
    "- Iteration count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class ReflectionState(TypedDict):\n",
    "    \"\"\"State for reflection loop.\"\"\"\n",
    "    messages: Annotated[list, add_messages]  # Conversation history\n",
    "    task: str                                 # Original task\n",
    "    draft: str                                # Current draft\n",
    "    critique: str                             # Latest critique\n",
    "    iteration: int                            # Current iteration\n",
    "\n",
    "print(\"State defined with: messages, task, draft, critique, iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Create the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0.7,  # Some creativity for writing\n",
    ")\n",
    "\n",
    "print(\"LLM configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Define the Generator Node\n",
    "\n",
    "The generator creates or revises the draft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "GENERATOR_PROMPT = \"\"\"You are a skilled writer. Your task is to write or revise content based on feedback.\n",
    "\n",
    "If this is the first draft, write a complete response to the task.\n",
    "If you have received critique, revise your draft to address the feedback.\n",
    "\n",
    "Focus on:\n",
    "- Clarity and conciseness\n",
    "- Accuracy and completeness\n",
    "- Engaging and professional tone\n",
    "\"\"\"\n",
    "\n",
    "def generate_node(state: ReflectionState) -> dict:\n",
    "    \"\"\"Generate or revise the draft.\"\"\"\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "    task = state[\"task\"]\n",
    "    draft = state.get(\"draft\", \"\")\n",
    "    critique = state.get(\"critique\", \"\")\n",
    "    \n",
    "    if iteration == 0:\n",
    "        # First draft\n",
    "        user_msg = f\"Write a response to this task:\\n\\n{task}\"\n",
    "    else:\n",
    "        # Revision based on critique\n",
    "        user_msg = f\"\"\"Revise this draft based on the critique.\n",
    "\n",
    "ORIGINAL TASK: {task}\n",
    "\n",
    "CURRENT DRAFT:\n",
    "{draft}\n",
    "\n",
    "CRITIQUE:\n",
    "{critique}\n",
    "\n",
    "Please provide an improved version addressing the feedback.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=GENERATOR_PROMPT),\n",
    "        HumanMessage(content=user_msg)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    new_draft = response.content\n",
    "    \n",
    "    print(f\"\\n=== Generation (iteration {iteration + 1}) ===\")\n",
    "    print(new_draft[:200] + \"...\" if len(new_draft) > 200 else new_draft)\n",
    "    \n",
    "    return {\n",
    "        \"draft\": new_draft,\n",
    "        \"iteration\": iteration + 1,\n",
    "        \"messages\": [AIMessage(content=f\"Draft {iteration + 1}: {new_draft}\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Define the Critique Node\n",
    "\n",
    "The critique node evaluates the draft and provides feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITIQUE_PROMPT = \"\"\"You are a thoughtful editor. Review the draft and provide constructive feedback.\n",
    "\n",
    "Evaluate:\n",
    "1. Does it fully address the task?\n",
    "2. Is it clear and well-organized?\n",
    "3. Are there any errors or areas for improvement?\n",
    "4. Is the tone appropriate?\n",
    "\n",
    "If the draft is excellent and needs no changes, respond with exactly: \"APPROVED\"\n",
    "Otherwise, provide specific, actionable feedback for improvement.\n",
    "\"\"\"\n",
    "\n",
    "def critique_node(state: ReflectionState) -> dict:\n",
    "    \"\"\"Critique the current draft.\"\"\"\n",
    "    task = state[\"task\"]\n",
    "    draft = state[\"draft\"]\n",
    "    iteration = state[\"iteration\"]\n",
    "    \n",
    "    user_msg = f\"\"\"Review this draft for the given task.\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "DRAFT:\n",
    "{draft}\n",
    "\n",
    "Provide your critique or respond with \"APPROVED\" if no changes are needed.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=user_msg)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    critique = response.content\n",
    "    \n",
    "    print(f\"\\n=== Critique (iteration {iteration}) ===\")\n",
    "    print(critique[:200] + \"...\" if len(critique) > 200 else critique)\n",
    "    \n",
    "    return {\n",
    "        \"critique\": critique,\n",
    "        \"messages\": [AIMessage(content=f\"Critique {iteration}: {critique}\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Define the Routing Logic\n",
    "\n",
    "Decide whether to continue revising or finish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 3\n",
    "\n",
    "def should_continue(state: ReflectionState) -> str:\n",
    "    \"\"\"Decide whether to continue refining or finish.\"\"\"\n",
    "    iteration = state[\"iteration\"]\n",
    "    critique = state.get(\"critique\", \"\")\n",
    "    \n",
    "    # Stop if approved\n",
    "    if \"APPROVED\" in critique.upper():\n",
    "        print(\"\\n✓ Draft approved!\")\n",
    "        return \"end\"\n",
    "    \n",
    "    # Stop if max iterations reached\n",
    "    if iteration >= MAX_ITERATIONS:\n",
    "        print(f\"\\n✓ Max iterations ({MAX_ITERATIONS}) reached\")\n",
    "        return \"end\"\n",
    "    \n",
    "    print(f\"\\n→ Continuing to iteration {iteration + 1}\")\n",
    "    return \"generate\"\n",
    "\n",
    "print(f\"Max iterations: {MAX_ITERATIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 6: Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build the reflection graph\n",
    "workflow = StateGraph(ReflectionState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"generate\", generate_node)\n",
    "workflow.add_node(\"critique\", critique_node)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"generate\")       # Start by generating\n",
    "workflow.add_edge(\"generate\", \"critique\")  # Then critique\n",
    "workflow.add_conditional_edges(\n",
    "    \"critique\",\n",
    "    should_continue,\n",
    "    {\"generate\": \"generate\", \"end\": END}\n",
    ")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"Reflection graph compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not render: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 7: Run the Reflection Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the reflection loop\n",
    "task = \"Write a brief explanation of what LangGraph is and why it's useful for building AI agents.\"\n",
    "\n",
    "print(f\"Task: {task}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"task\": task,\n",
    "    \"messages\": [],\n",
    "    \"draft\": \"\",\n",
    "    \"critique\": \"\",\n",
    "    \"iteration\": 0\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DRAFT:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"draft\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Step 8: Test with Different Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_on_task(task: str) -> str:\n",
    "    \"\"\"Run reflection loop on a task and return final draft.\"\"\"\n",
    "    result = graph.invoke({\n",
    "        \"task\": task,\n",
    "        \"messages\": [],\n",
    "        \"draft\": \"\",\n",
    "        \"critique\": \"\",\n",
    "        \"iteration\": 0\n",
    "    })\n",
    "    return result[\"draft\"]\n",
    "\n",
    "# Test with a different task\n",
    "email_task = \"Write a professional email declining a meeting invitation due to a scheduling conflict.\"\n",
    "\n",
    "print(f\"Task: {email_task}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_email = reflect_on_task(email_task)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EMAIL:\")\n",
    "print(\"=\"*60)\n",
    "print(final_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Reflection Agent\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "# === State ===\n",
    "class ReflectionState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    task: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    iteration: int\n",
    "\n",
    "# === LLM ===\n",
    "config = LocalAgentConfig()\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# === Nodes ===\n",
    "def generate(state: ReflectionState) -> dict:\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "    if iteration == 0:\n",
    "        prompt = f\"Write a response: {state['task']}\"\n",
    "    else:\n",
    "        prompt = f\"Revise based on critique:\\nDraft: {state['draft']}\\nCritique: {state['critique']}\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"draft\": response.content, \"iteration\": iteration + 1}\n",
    "\n",
    "def critique(state: ReflectionState) -> dict:\n",
    "    prompt = f\"Critique this draft (say APPROVED if perfect):\\n{state['draft']}\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"critique\": response.content}\n",
    "\n",
    "def should_continue(state: ReflectionState) -> str:\n",
    "    if \"APPROVED\" in state.get(\"critique\", \"\").upper():\n",
    "        return \"end\"\n",
    "    if state[\"iteration\"] >= 3:\n",
    "        return \"end\"\n",
    "    return \"generate\"\n",
    "\n",
    "# === Graph ===\n",
    "workflow = StateGraph(ReflectionState)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"critique\", critique)\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"critique\")\n",
    "workflow.add_conditional_edges(\"critique\", should_continue, {\"generate\": \"generate\", \"end\": END})\n",
    "graph = workflow.compile()\n",
    "\n",
    "# === Use ===\n",
    "result = graph.invoke({\"task\": \"Explain recursion in 2 sentences.\", \"messages\": [], \"draft\": \"\", \"critique\": \"\", \"iteration\": 0})\n",
    "print(result[\"draft\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Key Concepts Recap\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Generate** | Create initial or revised content |\n",
    "| **Critique** | Evaluate and provide feedback |\n",
    "| **Conditional Edge** | Route based on critique result |\n",
    "| **Iteration Limit** | Prevent infinite loops |\n",
    "| **Approval Signal** | Explicit signal to stop refining |\n",
    "\n",
    "## Variations\n",
    "\n",
    "1. **Two-model reflection**: Use a stronger model for critique\n",
    "2. **Structured feedback**: Use JSON for specific improvement areas\n",
    "3. **Tool-augmented**: Add research tools to fact-check\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In [Tutorial 06: Plan and Execute](06_plan_and_execute.ipynb), you'll learn:\n",
    "- Breaking complex tasks into steps\n",
    "- Planning before executing\n",
    "- Re-planning based on results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
