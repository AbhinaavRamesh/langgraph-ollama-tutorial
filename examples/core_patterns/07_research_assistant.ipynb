{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 07: Research Assistant\n",
    "\n",
    "This capstone tutorial combines all patterns from previous tutorials into a comprehensive research assistant agent.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Multi-step planning**: Breaking research tasks into phases\n",
    "- **Tool integration**: Search, retrieve, and analyze\n",
    "- **Reflection**: Self-critique and improve findings\n",
    "- **Synthesis**: Combine results into coherent reports\n",
    "\n",
    "By the end, you'll have a full-featured research agent that plans, searches, reflects, and synthesizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## Architecture Overview\n\nThe research assistant combines patterns from:\n- **Tutorial 02**: Tool calling for search and retrieval\n- **Tutorial 03**: Memory for tracking research context\n- **Tutorial 05**: Reflection for quality improvement\n- **Tutorial 06**: Plan-and-execute for structured workflow\n\n### The Research Pipeline\n\n![Research Assistant Graph](../docs/images/07-research-assistant-graph.png)\n\nThe pipeline orchestrates: Planner → Researcher → Analyzer → Reflector → Synthesizer, with loops back to Researcher if gaps are found."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Graph Visualization\n",
    "\n",
    "![Research Assistant Graph](../docs/images/07-research-assistant-graph.png)\n",
    "\n",
    "The research assistant orchestrates planning, searching, analyzing, reflecting, and synthesizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "config = LocalAgentConfig()\n",
    "print(f\"Ollama: {config.ollama.base_url}\")\n",
    "print(f\"Model: {config.ollama.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 1: Define Research State\n",
    "\n",
    "The state tracks the full research lifecycle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Tuple, Optional\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    \"\"\"State for the research assistant.\"\"\"\n",
    "    messages: Annotated[list, add_messages]  # Conversation history\n",
    "    \n",
    "    # Task\n",
    "    query: str                               # Research query\n",
    "    \n",
    "    # Planning\n",
    "    research_plan: List[str]                 # Research steps\n",
    "    current_step: int                        # Current step index\n",
    "    \n",
    "    # Research\n",
    "    sources: Annotated[List[dict], operator.add]  # Gathered sources\n",
    "    findings: Annotated[List[str], operator.add]  # Key findings\n",
    "    \n",
    "    # Reflection\n",
    "    critique: str                            # Current critique\n",
    "    gaps: List[str]                          # Identified gaps\n",
    "    iteration: int                           # Reflection iteration\n",
    "    \n",
    "    # Output\n",
    "    report: str                              # Final synthesized report\n",
    "\n",
    "print(\"Research state defined with planning, research, reflection, and output tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Create the LLM and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(\n",
    "    model=config.ollama.model,\n",
    "    base_url=config.ollama.base_url,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Simulated research tools (in production, connect to real APIs)\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information on a topic.\"\"\"\n",
    "    # Simulated search results\n",
    "    results = {\n",
    "        \"langgraph\": [\n",
    "            {\"title\": \"LangGraph Documentation\", \"snippet\": \"LangGraph is a library for building stateful, multi-actor applications with LLMs.\", \"url\": \"https://langchain-ai.github.io/langgraph/\"},\n",
    "            {\"title\": \"LangGraph Tutorial\", \"snippet\": \"Build agentic workflows with cycles, persistence, and human-in-the-loop.\", \"url\": \"https://python.langchain.com/docs/langgraph\"},\n",
    "        ],\n",
    "        \"agents\": [\n",
    "            {\"title\": \"AI Agents Overview\", \"snippet\": \"AI agents are autonomous systems that can perceive, reason, and act.\", \"url\": \"https://example.com/agents\"},\n",
    "            {\"title\": \"Building Agents with LLMs\", \"snippet\": \"Modern agents combine LLMs with tools and memory for complex tasks.\", \"url\": \"https://example.com/llm-agents\"},\n",
    "        ],\n",
    "        \"default\": [\n",
    "            {\"title\": f\"Search results for: {query}\", \"snippet\": f\"Information about {query}...\", \"url\": f\"https://example.com/{query.replace(' ', '-')}\"},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Match query to results\n",
    "    for key in results:\n",
    "        if key.lower() in query.lower():\n",
    "            return json.dumps(results[key])\n",
    "    return json.dumps(results[\"default\"])\n",
    "\n",
    "@tool\n",
    "def fetch_document(url: str) -> str:\n",
    "    \"\"\"Fetch and extract content from a URL.\"\"\"\n",
    "    # Simulated document content\n",
    "    content = f\"\"\"Document from {url}:\n",
    "    \n",
    "This is simulated content that would be fetched from the URL.\n",
    "In a production system, this would use requests/httpx to fetch real content.\n",
    "\n",
    "Key points:\n",
    "- Point 1: Important information about the topic\n",
    "- Point 2: Additional relevant details\n",
    "- Point 3: Supporting evidence and examples\n",
    "\"\"\"\n",
    "    return content\n",
    "\n",
    "@tool\n",
    "def take_notes(topic: str, notes: str) -> str:\n",
    "    \"\"\"Record research notes on a topic.\"\"\"\n",
    "    return f\"Notes recorded for '{topic}': {notes}\"\n",
    "\n",
    "tools = [web_search, fetch_document, take_notes]\n",
    "tools_by_name = {t.name: t for t in tools}\n",
    "\n",
    "# LLM with tools\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(f\"Configured with {len(tools)} research tools: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Create Planning Node\n",
    "\n",
    "The planner creates a structured research plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "import re\n",
    "\n",
    "PLANNER_PROMPT = \"\"\"You are a research planner. Create a structured plan for researching the given topic.\n",
    "\n",
    "Return your plan as a JSON array of 3-5 research steps. Each step should be a specific, actionable task.\n",
    "\n",
    "Example:\n",
    "[\"Search for overview and definitions\", \"Find specific examples and case studies\", \"Gather expert opinions and analysis\", \"Identify key trends and future directions\"]\n",
    "\n",
    "Return ONLY the JSON array.\n",
    "\"\"\"\n",
    "\n",
    "def planner_node(state: ResearchState) -> dict:\n",
    "    \"\"\"Create a research plan.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=PLANNER_PROMPT),\n",
    "        HumanMessage(content=f\"Create a research plan for: {query}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    content = response.content\n",
    "    \n",
    "    # Parse JSON plan\n",
    "    try:\n",
    "        match = re.search(r'\\[.*?\\]', content, re.DOTALL)\n",
    "        if match:\n",
    "            plan = json.loads(match.group())\n",
    "        else:\n",
    "            plan = [line.strip() for line in content.split('\\n') if line.strip()]\n",
    "    except json.JSONDecodeError:\n",
    "        plan = [f\"Research: {query}\"]\n",
    "    \n",
    "    print(f\"\\n=== RESEARCH PLAN ===\")\n",
    "    for i, step in enumerate(plan):\n",
    "        print(f\"  {i+1}. {step}\")\n",
    "    \n",
    "    return {\n",
    "        \"research_plan\": plan,\n",
    "        \"current_step\": 0,\n",
    "        \"iteration\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Create Researcher Node\n",
    "\n",
    "The researcher executes the current step using tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "RESEARCHER_PROMPT = \"\"\"You are a research assistant. Execute the current research step by using the available tools.\n",
    "\n",
    "Available tools:\n",
    "- web_search: Search for information on a topic\n",
    "- fetch_document: Get content from a URL\n",
    "- take_notes: Record important findings\n",
    "\n",
    "Use tools to gather information, then summarize your findings.\n",
    "\"\"\"\n",
    "\n",
    "def researcher_node(state: ResearchState) -> dict:\n",
    "    \"\"\"Execute current research step with tools.\"\"\"\n",
    "    plan = state[\"research_plan\"]\n",
    "    current_step = state[\"current_step\"]\n",
    "    query = state[\"query\"]\n",
    "    gaps = state.get(\"gaps\", [])\n",
    "    \n",
    "    if current_step >= len(plan):\n",
    "        return {}\n",
    "    \n",
    "    step = plan[current_step]\n",
    "    \n",
    "    # Include any identified gaps\n",
    "    gap_context = \"\"\n",
    "    if gaps:\n",
    "        gap_context = f\"\\n\\nAlso address these gaps: {', '.join(gaps)}\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=RESEARCHER_PROMPT),\n",
    "        HumanMessage(content=f\"Research topic: {query}\\n\\nCurrent step: {step}{gap_context}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    print(f\"\\n=== RESEARCH STEP {current_step + 1}: {step} ===\")\n",
    "    \n",
    "    # Execute any tool calls\n",
    "    new_sources = []\n",
    "    new_findings = []\n",
    "    \n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        for tc in response.tool_calls:\n",
    "            tool_name = tc['name']\n",
    "            tool_args = tc['args']\n",
    "            print(f\"  Tool: {tool_name}({tool_args})\")\n",
    "            \n",
    "            result = tools_by_name[tool_name].invoke(tool_args)\n",
    "            \n",
    "            # Track sources from web_search\n",
    "            if tool_name == 'web_search':\n",
    "                try:\n",
    "                    sources = json.loads(result)\n",
    "                    new_sources.extend(sources)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            new_findings.append(f\"{step}: {result[:200]}...\" if len(result) > 200 else f\"{step}: {result}\")\n",
    "    else:\n",
    "        # No tool calls, use response directly\n",
    "        new_findings.append(f\"{step}: {response.content[:200]}...\" if len(response.content) > 200 else f\"{step}: {response.content}\")\n",
    "    \n",
    "    print(f\"  Found {len(new_sources)} sources, {len(new_findings)} findings\")\n",
    "    \n",
    "    return {\n",
    "        \"sources\": new_sources,\n",
    "        \"findings\": new_findings,\n",
    "        \"current_step\": current_step + 1,\n",
    "        \"gaps\": []  # Clear gaps after addressing\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Create Analyzer Node\n",
    "\n",
    "The analyzer processes and organizes findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYZER_PROMPT = \"\"\"You are a research analyst. Review the gathered findings and identify key insights.\n",
    "\n",
    "Organize the information into:\n",
    "1. Key facts and definitions\n",
    "2. Important examples or evidence\n",
    "3. Expert perspectives\n",
    "4. Patterns and trends\n",
    "\n",
    "Be concise but comprehensive.\n",
    "\"\"\"\n",
    "\n",
    "def analyzer_node(state: ResearchState) -> dict:\n",
    "    \"\"\"Analyze and organize findings.\"\"\"\n",
    "    findings = state.get(\"findings\", [])\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    if not findings:\n",
    "        return {\"findings\": [\"No findings to analyze\"]}\n",
    "    \n",
    "    findings_text = \"\\n\".join(findings)\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=ANALYZER_PROMPT),\n",
    "        HumanMessage(content=f\"Research topic: {query}\\n\\nFindings:\\n{findings_text}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    analysis = response.content\n",
    "    \n",
    "    print(f\"\\n=== ANALYSIS ===\")\n",
    "    print(analysis[:300] + \"...\" if len(analysis) > 300 else analysis)\n",
    "    \n",
    "    return {\n",
    "        \"findings\": [f\"Analysis: {analysis}\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 6: Create Reflector Node\n",
    "\n",
    "The reflector critiques research quality and identifies gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECTOR_PROMPT = \"\"\"You are a research quality reviewer. Evaluate the research findings and identify any gaps.\n",
    "\n",
    "Consider:\n",
    "1. Is the topic fully covered?\n",
    "2. Are there missing perspectives?\n",
    "3. Is the evidence sufficient?\n",
    "4. Are there unanswered questions?\n",
    "\n",
    "If the research is comprehensive, respond with: \"COMPLETE\"\n",
    "Otherwise, list the specific gaps that need to be addressed as a JSON array.\n",
    "\n",
    "Example gaps: [\"Missing historical context\", \"Need more expert opinions\", \"Lacks practical examples\"]\n",
    "\"\"\"\n",
    "\n",
    "MAX_ITERATIONS = 2\n",
    "\n",
    "def reflector_node(state: ResearchState) -> dict:\n",
    "    \"\"\"Critique research and identify gaps.\"\"\"\n",
    "    findings = state.get(\"findings\", [])\n",
    "    query = state[\"query\"]\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "    \n",
    "    findings_text = \"\\n\".join(findings[-5:])  # Last 5 findings\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTOR_PROMPT),\n",
    "        HumanMessage(content=f\"Research topic: {query}\\n\\nCurrent findings:\\n{findings_text}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    critique = response.content\n",
    "    \n",
    "    print(f\"\\n=== REFLECTION (iteration {iteration + 1}) ===\")\n",
    "    print(critique[:200] + \"...\" if len(critique) > 200 else critique)\n",
    "    \n",
    "    # Parse gaps\n",
    "    gaps = []\n",
    "    if \"COMPLETE\" not in critique.upper():\n",
    "        try:\n",
    "            match = re.search(r'\\[.*?\\]', critique, re.DOTALL)\n",
    "            if match:\n",
    "                gaps = json.loads(match.group())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return {\n",
    "        \"critique\": critique,\n",
    "        \"gaps\": gaps,\n",
    "        \"iteration\": iteration + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 7: Create Synthesizer Node\n",
    "\n",
    "The synthesizer creates the final research report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNTHESIZER_PROMPT = \"\"\"You are a research report writer. Create a comprehensive, well-organized report from the research findings.\n",
    "\n",
    "Structure your report with:\n",
    "1. Executive Summary\n",
    "2. Key Findings\n",
    "3. Analysis\n",
    "4. Conclusions\n",
    "5. Sources (if available)\n",
    "\n",
    "Write clearly and professionally.\n",
    "\"\"\"\n",
    "\n",
    "def synthesizer_node(state: ResearchState) -> dict:\n",
    "    \"\"\"Create final research report.\"\"\"\n",
    "    findings = state.get(\"findings\", [])\n",
    "    sources = state.get(\"sources\", [])\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    findings_text = \"\\n\".join(findings)\n",
    "    sources_text = \"\\n\".join([f\"- {s.get('title', 'Unknown')}: {s.get('url', '')}\" for s in sources[:5]])\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=SYNTHESIZER_PROMPT),\n",
    "        HumanMessage(content=f\"Research topic: {query}\\n\\nFindings:\\n{findings_text}\\n\\nSources:\\n{sources_text}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    report = response.content\n",
    "    \n",
    "    print(f\"\\n=== FINAL REPORT ===\")\n",
    "    print(report)\n",
    "    \n",
    "    return {\"report\": report}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 8: Define Routing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_research(state: ResearchState) -> str:\n",
    "    \"\"\"Route after research step.\"\"\"\n",
    "    current_step = state[\"current_step\"]\n",
    "    plan = state[\"research_plan\"]\n",
    "    \n",
    "    if current_step < len(plan):\n",
    "        return \"researcher\"  # More steps\n",
    "    return \"analyzer\"  # All steps done\n",
    "\n",
    "def route_after_reflection(state: ResearchState) -> str:\n",
    "    \"\"\"Route after reflection.\"\"\"\n",
    "    gaps = state.get(\"gaps\", [])\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "    critique = state.get(\"critique\", \"\")\n",
    "    \n",
    "    # Complete if approved or max iterations\n",
    "    if \"COMPLETE\" in critique.upper():\n",
    "        print(\"\\n✓ Research complete!\")\n",
    "        return \"synthesizer\"\n",
    "    \n",
    "    if iteration >= MAX_ITERATIONS:\n",
    "        print(f\"\\n✓ Max iterations ({MAX_ITERATIONS}) reached\")\n",
    "        return \"synthesizer\"\n",
    "    \n",
    "    if gaps:\n",
    "        print(f\"\\n→ Addressing {len(gaps)} gaps...\")\n",
    "        return \"researcher\"  # Go back and fill gaps\n",
    "    \n",
    "    return \"synthesizer\"\n",
    "\n",
    "print(\"Routing logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 9: Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build the research assistant graph\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"researcher\", researcher_node)\n",
    "workflow.add_node(\"analyzer\", analyzer_node)\n",
    "workflow.add_node(\"reflector\", reflector_node)\n",
    "workflow.add_node(\"synthesizer\", synthesizer_node)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"researcher\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"researcher\",\n",
    "    route_after_research,\n",
    "    {\"researcher\": \"researcher\", \"analyzer\": \"analyzer\"}\n",
    ")\n",
    "workflow.add_edge(\"analyzer\", \"reflector\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"reflector\",\n",
    "    route_after_reflection,\n",
    "    {\"researcher\": \"researcher\", \"synthesizer\": \"synthesizer\"}\n",
    ")\n",
    "workflow.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"Research assistant graph compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not render: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Step 10: Run the Research Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a research query\n",
    "query = \"What is LangGraph and how is it used for building AI agents?\"\n",
    "\n",
    "print(f\"Research Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"query\": query,\n",
    "    \"messages\": [],\n",
    "    \"research_plan\": [],\n",
    "    \"current_step\": 0,\n",
    "    \"sources\": [],\n",
    "    \"findings\": [],\n",
    "    \"critique\": \"\",\n",
    "    \"gaps\": [],\n",
    "    \"iteration\": 0,\n",
    "    \"report\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESEARCH COMPLETE\")\n",
    "print(f\"Sources gathered: {len(result['sources'])}\")\n",
    "print(f\"Findings recorded: {len(result['findings'])}\")\n",
    "print(f\"Iterations: {result['iteration']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def research(query: str) -> str:\n",
    "    \"\"\"Run research on a query and return the report.\"\"\"\n",
    "    result = graph.invoke({\n",
    "        \"query\": query,\n",
    "        \"messages\": [],\n",
    "        \"research_plan\": [],\n",
    "        \"current_step\": 0,\n",
    "        \"sources\": [],\n",
    "        \"findings\": [],\n",
    "        \"critique\": \"\",\n",
    "        \"gaps\": [],\n",
    "        \"iteration\": 0,\n",
    "        \"report\": \"\"\n",
    "    })\n",
    "    return result[\"report\"]\n",
    "\n",
    "# Try another query\n",
    "report = research(\"What are the key differences between ReAct agents and Plan-and-Execute agents?\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL REPORT:\")\n",
    "print(\"=\" * 60)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Adding Persistence\n",
    "\n",
    "For longer research sessions, add memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Compile with persistence\n",
    "memory = MemorySaver()\n",
    "persistent_graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Run with thread ID\n",
    "config = {\"configurable\": {\"thread_id\": \"research-session-1\"}}\n",
    "\n",
    "result = persistent_graph.invoke(\n",
    "    {\n",
    "        \"query\": \"How does LangGraph handle state management?\",\n",
    "        \"messages\": [],\n",
    "        \"research_plan\": [],\n",
    "        \"current_step\": 0,\n",
    "        \"sources\": [],\n",
    "        \"findings\": [],\n",
    "        \"critique\": \"\",\n",
    "        \"gaps\": [],\n",
    "        \"iteration\": 0,\n",
    "        \"report\": \"\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"Research with persistence complete!\")\n",
    "print(f\"Report length: {len(result['report'])} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Research Assistant\n",
    "\n",
    "import json\n",
    "import re\n",
    "import operator\n",
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph_ollama_local import LocalAgentConfig\n",
    "\n",
    "# === State ===\n",
    "class ResearchState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    query: str\n",
    "    research_plan: List[str]\n",
    "    current_step: int\n",
    "    sources: Annotated[List[dict], operator.add]\n",
    "    findings: Annotated[List[str], operator.add]\n",
    "    critique: str\n",
    "    gaps: List[str]\n",
    "    iteration: int\n",
    "    report: str\n",
    "\n",
    "# === LLM & Tools ===\n",
    "config = LocalAgentConfig()\n",
    "llm = ChatOllama(model=config.ollama.model, base_url=config.ollama.base_url, temperature=0)\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web.\"\"\"\n",
    "    return json.dumps([{\"title\": f\"Results for {query}\", \"snippet\": f\"Info about {query}\"}])\n",
    "\n",
    "tools = [web_search]\n",
    "tools_by_name = {t.name: t for t in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# === Nodes ===\n",
    "def planner(state):\n",
    "    response = llm.invoke([HumanMessage(content=f\"Create 3-5 research steps (JSON array): {state['query']}\")])\n",
    "    try:\n",
    "        plan = json.loads(re.search(r'\\[.*?\\]', response.content, re.DOTALL).group())\n",
    "    except:\n",
    "        plan = [f\"Research: {state['query']}\"]\n",
    "    return {\"research_plan\": plan, \"current_step\": 0, \"iteration\": 0}\n",
    "\n",
    "def researcher(state):\n",
    "    if state[\"current_step\"] >= len(state[\"research_plan\"]):\n",
    "        return {}\n",
    "    step = state[\"research_plan\"][state[\"current_step\"]]\n",
    "    response = llm_with_tools.invoke([HumanMessage(content=f\"Research: {step}\")])\n",
    "    return {\"findings\": [f\"{step}: {response.content[:200]}\"], \"current_step\": state[\"current_step\"] + 1, \"gaps\": []}\n",
    "\n",
    "def analyzer(state):\n",
    "    response = llm.invoke([HumanMessage(content=f\"Analyze: {state['findings'][-3:]}\")])\n",
    "    return {\"findings\": [f\"Analysis: {response.content}\"]}\n",
    "\n",
    "def reflector(state):\n",
    "    response = llm.invoke([HumanMessage(content=f\"Critique (say COMPLETE if done): {state['findings'][-1]}\")])\n",
    "    gaps = []\n",
    "    if \"COMPLETE\" not in response.content.upper():\n",
    "        try:\n",
    "            match = re.search(r'\\[.*?\\]', response.content, re.DOTALL)\n",
    "            if match:\n",
    "                gaps = json.loads(match.group())\n",
    "        except:\n",
    "            pass\n",
    "    return {\"critique\": response.content, \"gaps\": gaps, \"iteration\": state[\"iteration\"] + 1}\n",
    "\n",
    "def synthesizer(state):\n",
    "    response = llm.invoke([HumanMessage(content=f\"Write report on {state['query']}:\\n{state['findings']}\")])\n",
    "    return {\"report\": response.content}\n",
    "\n",
    "def route_research(state):\n",
    "    return \"researcher\" if state[\"current_step\"] < len(state[\"research_plan\"]) else \"analyzer\"\n",
    "\n",
    "def route_reflection(state):\n",
    "    if \"COMPLETE\" in state.get(\"critique\", \"\").upper() or state[\"iteration\"] >= 2:\n",
    "        return \"synthesizer\"\n",
    "    return \"researcher\" if state.get(\"gaps\") else \"synthesizer\"\n",
    "\n",
    "# === Graph ===\n",
    "workflow = StateGraph(ResearchState)\n",
    "workflow.add_node(\"planner\", planner)\n",
    "workflow.add_node(\"researcher\", researcher)\n",
    "workflow.add_node(\"analyzer\", analyzer)\n",
    "workflow.add_node(\"reflector\", reflector)\n",
    "workflow.add_node(\"synthesizer\", synthesizer)\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"researcher\")\n",
    "workflow.add_conditional_edges(\"researcher\", route_research, {\"researcher\": \"researcher\", \"analyzer\": \"analyzer\"})\n",
    "workflow.add_edge(\"analyzer\", \"reflector\")\n",
    "workflow.add_conditional_edges(\"reflector\", route_reflection, {\"researcher\": \"researcher\", \"synthesizer\": \"synthesizer\"})\n",
    "workflow.add_edge(\"synthesizer\", END)\n",
    "graph = workflow.compile()\n",
    "\n",
    "# === Use ===\n",
    "result = graph.invoke({\n",
    "    \"query\": \"What is LangGraph?\",\n",
    "    \"messages\": [], \"research_plan\": [], \"current_step\": 0,\n",
    "    \"sources\": [], \"findings\": [], \"critique\": \"\", \"gaps\": [],\n",
    "    \"iteration\": 0, \"report\": \"\"\n",
    "})\n",
    "print(result[\"report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Key Concepts Recap\n",
    "\n",
    "| Pattern | How It's Used |\n",
    "|---------|---------------|\n",
    "| **Tool Calling** | web_search, fetch_document for gathering info |\n",
    "| **Planning** | Structured research steps before execution |\n",
    "| **Sequential Execution** | Process each research step in order |\n",
    "| **Reflection** | Critique findings, identify gaps |\n",
    "| **Conditional Routing** | Loop back if gaps found |\n",
    "| **Synthesis** | Combine all findings into final report |\n",
    "\n",
    "## Production Enhancements\n",
    "\n",
    "For a production research assistant:\n",
    "\n",
    "1. **Real search APIs**: Integrate Tavily, SerpAPI, or Brave Search\n",
    "2. **Document loaders**: Use LangChain document loaders for PDFs, web pages\n",
    "3. **Vector storage**: Store and retrieve research with embeddings\n",
    "4. **Human-in-the-loop**: Add approval for research directions\n",
    "5. **Streaming**: Stream progress updates to the user\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the LangGraph tutorial series! You now know how to build:\n",
    "- Basic chatbots\n",
    "- Tool-calling ReAct agents\n",
    "- Persistent memory systems\n",
    "- Human-in-the-loop workflows\n",
    "- Self-improving reflection loops\n",
    "- Plan-and-execute agents\n",
    "- Full research assistants\n",
    "\n",
    "Happy building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}